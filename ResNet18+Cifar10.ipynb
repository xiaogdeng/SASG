{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result for **ResNet18+Cifar10**\n",
    "### This notebook is for seed $42$. The results in the paper are the average of $5$ times with seeds in $[1, 19, 31, 42, 80]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import MultipleLocator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"resnet in pytorch\n",
    "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun.\n",
    "    Deep Residual Learning for Image Recognition\n",
    "    https://arxiv.org/abs/1512.03385v1\n",
    "\"\"\"\n",
    "class BasicBlock(nn.Module):\n",
    "    \"\"\"Basic Block for resnet 18 and resnet 34\n",
    "    \"\"\"\n",
    "    expansion = 1\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels * BasicBlock.expansion, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
    "        )\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        if stride != 1 or in_channels != BasicBlock.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * BasicBlock.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n",
    "\n",
    "\n",
    "class BottleNeck(nn.Module):\n",
    "    \"\"\"Residual block for resnet over 50 layers\n",
    "    \"\"\"\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BottleNeck, self).__init__()\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, stride=stride, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
    "        )\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * BottleNeck.expansion, stride=stride, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * BottleNeck.expansion)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_block, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True))\n",
    "        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)\n",
    "        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)\n",
    "        self.conv4_x = self._make_layer(block, 256, num_block[2], 2)\n",
    "        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        \"\"\"make resnet layers(by layer i didnt mean this 'layer' was the\n",
    "        same as a neuron netowork layer, ex. conv layer), one layer may\n",
    "        contain more than one residual block\n",
    "        Args:\n",
    "            block: block type, basic block or bottle neck block\n",
    "            out_channels: output depth channel number of this layer\n",
    "            num_blocks: how many blocks per layer\n",
    "            stride: the stride of the first block of this layer\n",
    "        Return:\n",
    "            return a resnet layer\n",
    "        \"\"\"\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        output = self.conv1(x)\n",
    "        output = self.conv2_x(output)\n",
    "        output = self.conv3_x(output)\n",
    "        output = self.conv4_x(output)\n",
    "        output = self.conv5_x(output)\n",
    "        output = self.avg_pool(output)\n",
    "        output = output.view(output.size(0), -1)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "def resnet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "\n",
    "def resnet18_copy():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loaders(dataset_name, n_workers, batch_size):\n",
    "    train_data, test_data = load_data(dataset_name)\n",
    "    train_loader_workers = dict()\n",
    "    n = len(train_data)\n",
    "    \n",
    "    # preparing iterators for workers\n",
    "    indices = np.arange(n)\n",
    "    np.random.shuffle(indices)\n",
    "    a = np.int(np.floor(n / n_workers))\n",
    "    top_ind = a * n_workers\n",
    "    seq = range(a, top_ind, a)\n",
    "    split = np.split(indices[:top_ind], seq)\n",
    "    b = 0\n",
    "    for ind in split:\n",
    "        train_loader_workers[b] = DataLoader(Subset(train_data, ind), batch_size=batch_size, shuffle=True)\n",
    "        b = b + 1\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader_workers, test_loader\n",
    "\n",
    "\n",
    "def load_data(dataset_name):\n",
    "    if dataset_name == 'mnist':\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "        train_data = datasets.MNIST(root='./data', train=True,\n",
    "                                    download=True, transform=transform)\n",
    "        test_data = datasets.MNIST(root='./data', train=False,\n",
    "                                   download=True, transform=transform)\n",
    "\n",
    "    elif dataset_name == 'cifar10':\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "        train_data = datasets.CIFAR10(root='./data', train=True,\n",
    "                                      download=True, transform=transform_train)\n",
    "        test_data = datasets.CIFAR10(root='./data', train=False,\n",
    "                                     download=True, transform=transform_test)\n",
    "\n",
    "    elif dataset_name == 'cifar100':\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5070751592371323, 0.48654887331495095, 0.4409178433670343],\n",
    "                                 std=[0.5070751592371323, 0.48654887331495095, 0.4409178433670343])\n",
    "        ])\n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5070751592371323, 0.48654887331495095, 0.4409178433670343],\n",
    "                                 std=[0.5070751592371323, 0.48654887331495095, 0.4409178433670343])\n",
    "        ])\n",
    "        train_data = datasets.CIFAR100(root='./data', train=True,\n",
    "                                       download=True, transform=transform_train)\n",
    "        test_data = datasets.CIFAR100(root='./data', train=False,\n",
    "                                      download=True, transform=transform_test)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(dataset_name + ' is not known.')\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "RANDOM_SEED = 42\n",
    "lr = 0.01\n",
    "BATCH_SIZE = 10\n",
    "NUM_EPOCHS = 30\n",
    "NUM_WORKERS = 10\n",
    "D = 10  # compute thread\n",
    "h = 0.01  # sparse level 99%\n",
    "\n",
    "if not os.path.exists('result'):\n",
    "    os.mkdir('result')\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "    \n",
    "def evaluate_accuracy(model, data_iter, device):\n",
    "    correct = 0\n",
    "    for image, label in data_iter:\n",
    "        image, label = image.to(device), label.to(device)\n",
    "        output = model(image)\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        correct += pred.eq(label.view_as(pred)).sum().item()\n",
    "    accuracy = 100. * correct / len(data_iter.dataset)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Top_k sparse\n",
    "def prep_grad(x):\n",
    "    x_flat = torch.unsqueeze(x, 0).flatten()\n",
    "    dim = x.shape\n",
    "    d = x_flat.shape[0]\n",
    "    return x_flat, dim, d\n",
    "\n",
    "\n",
    "def top_k_opt(x, h):\n",
    "    \"\"\"\n",
    "    :param x: vector to sparsify\n",
    "    :param h: density\n",
    "    :return: compressed vector\n",
    "    \"\"\"\n",
    "    x, dim, d = prep_grad(x)\n",
    "    # number of coordinates kept\n",
    "    r = int(np.maximum(1, np.floor(d * h)))\n",
    "    # positions of top_k coordinates\n",
    "    _, ind = torch.topk(torch.abs(x), r)\n",
    "    mask = torch.zeros_like(x)\n",
    "    mask[ind] = 1\n",
    "    t = mask * x\n",
    "    t = t.reshape(dim)\n",
    "    return t\n",
    "\n",
    "\n",
    "def compt(old, new):\n",
    "    result = 0.0\n",
    "    for i in range(len(old)):\n",
    "        result += ((old[i].view(-1) - new[i].view(-1)) ** 2).sum().item()\n",
    "    return result\n",
    "\n",
    "\n",
    "def adjust_lr(epoch_input):\n",
    "    lr_ad, lr_th = 0.01, 0.01\n",
    "    if epoch_input >= 20:\n",
    "        lr_ad = 0.001\n",
    "    if epoch_input > 20:\n",
    "        lr_th = 0.001\n",
    "    return lr_ad, lr_th"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SASG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SASG  lr:0.01--h:0.01--epoch:30--worker:10\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Number_parameter: 62\n",
      "Element_parameter: 11173962\n",
      "Element_parameter_sparse: 111723\n",
      "EPOCH:  1 learning rate:  0.01\n",
      "Epoch: 001/030 | Batch 0100/0500 | Cost: 1.6271\n",
      "Epoch: 001/030 | Batch 0200/0500 | Cost: 1.8239\n",
      "Epoch: 001/030 | Batch 0300/0500 | Cost: 1.2648\n",
      "Epoch: 001/030 | Batch 0400/0500 | Cost: 1.1663\n",
      "Epoch: 001/030 | Batch 0500/0500 | Cost: 0.6823\n",
      "epoch 1, skip_num 0, loss 1.4529, train acc 47.486%, test acc 62.610%\n",
      "EPOCH:  2 learning rate:  0.01\n",
      "Epoch: 002/030 | Batch 0100/0500 | Cost: 1.2413\n",
      "Epoch: 002/030 | Batch 0200/0500 | Cost: 1.5484\n",
      "Epoch: 002/030 | Batch 0300/0500 | Cost: 1.2059\n",
      "Epoch: 002/030 | Batch 0400/0500 | Cost: 0.4741\n",
      "Epoch: 002/030 | Batch 0500/0500 | Cost: 1.4950\n",
      "epoch 2, skip_num 9, loss 0.9308, train acc 67.454%, test acc 75.850%\n",
      "EPOCH:  3 learning rate:  0.01\n",
      "Epoch: 003/030 | Batch 0100/0500 | Cost: 0.5324\n",
      "Epoch: 003/030 | Batch 0200/0500 | Cost: 1.0553\n",
      "Epoch: 003/030 | Batch 0300/0500 | Cost: 1.0334\n",
      "Epoch: 003/030 | Batch 0400/0500 | Cost: 0.4664\n",
      "Epoch: 003/030 | Batch 0500/0500 | Cost: 1.0283\n",
      "epoch 3, skip_num 60, loss 0.7364, train acc 74.582%, test acc 79.780%\n",
      "EPOCH:  4 learning rate:  0.01\n",
      "Epoch: 004/030 | Batch 0100/0500 | Cost: 0.3794\n",
      "Epoch: 004/030 | Batch 0200/0500 | Cost: 0.6582\n",
      "Epoch: 004/030 | Batch 0300/0500 | Cost: 1.1893\n",
      "Epoch: 004/030 | Batch 0400/0500 | Cost: 0.4379\n",
      "Epoch: 004/030 | Batch 0500/0500 | Cost: 1.0456\n",
      "epoch 4, skip_num 171, loss 0.6234, train acc 78.520%, test acc 80.920%\n",
      "EPOCH:  5 learning rate:  0.01\n",
      "Epoch: 005/030 | Batch 0100/0500 | Cost: 0.4618\n",
      "Epoch: 005/030 | Batch 0200/0500 | Cost: 0.5200\n",
      "Epoch: 005/030 | Batch 0300/0500 | Cost: 0.1337\n",
      "Epoch: 005/030 | Batch 0400/0500 | Cost: 0.4346\n",
      "Epoch: 005/030 | Batch 0500/0500 | Cost: 0.7077\n",
      "epoch 5, skip_num 271, loss 0.5468, train acc 81.276%, test acc 83.140%\n",
      "EPOCH:  6 learning rate:  0.01\n",
      "Epoch: 006/030 | Batch 0100/0500 | Cost: 0.3506\n",
      "Epoch: 006/030 | Batch 0200/0500 | Cost: 0.3396\n",
      "Epoch: 006/030 | Batch 0300/0500 | Cost: 1.0766\n",
      "Epoch: 006/030 | Batch 0400/0500 | Cost: 0.7239\n",
      "Epoch: 006/030 | Batch 0500/0500 | Cost: 0.3095\n",
      "epoch 6, skip_num 380, loss 0.4900, train acc 83.004%, test acc 84.530%\n",
      "EPOCH:  7 learning rate:  0.01\n",
      "Epoch: 007/030 | Batch 0100/0500 | Cost: 0.1765\n",
      "Epoch: 007/030 | Batch 0200/0500 | Cost: 0.3649\n",
      "Epoch: 007/030 | Batch 0300/0500 | Cost: 0.2823\n",
      "Epoch: 007/030 | Batch 0400/0500 | Cost: 0.2106\n",
      "Epoch: 007/030 | Batch 0500/0500 | Cost: 0.6766\n",
      "epoch 7, skip_num 513, loss 0.4433, train acc 84.808%, test acc 85.200%\n",
      "EPOCH:  8 learning rate:  0.01\n",
      "Epoch: 008/030 | Batch 0100/0500 | Cost: 0.6357\n",
      "Epoch: 008/030 | Batch 0200/0500 | Cost: 0.2009\n",
      "Epoch: 008/030 | Batch 0300/0500 | Cost: 0.5950\n",
      "Epoch: 008/030 | Batch 0400/0500 | Cost: 0.6906\n",
      "Epoch: 008/030 | Batch 0500/0500 | Cost: 0.3056\n",
      "epoch 8, skip_num 646, loss 0.4059, train acc 85.990%, test acc 85.320%\n",
      "EPOCH:  9 learning rate:  0.01\n",
      "Epoch: 009/030 | Batch 0100/0500 | Cost: 0.5379\n",
      "Epoch: 009/030 | Batch 0200/0500 | Cost: 0.3476\n",
      "Epoch: 009/030 | Batch 0300/0500 | Cost: 0.2324\n",
      "Epoch: 009/030 | Batch 0400/0500 | Cost: 0.5326\n",
      "Epoch: 009/030 | Batch 0500/0500 | Cost: 0.1886\n",
      "epoch 9, skip_num 751, loss 0.3732, train acc 87.056%, test acc 86.920%\n",
      "EPOCH:  10 learning rate:  0.01\n",
      "Epoch: 010/030 | Batch 0100/0500 | Cost: 0.1083\n",
      "Epoch: 010/030 | Batch 0200/0500 | Cost: 0.2710\n",
      "Epoch: 010/030 | Batch 0300/0500 | Cost: 0.2023\n",
      "Epoch: 010/030 | Batch 0400/0500 | Cost: 0.3249\n",
      "Epoch: 010/030 | Batch 0500/0500 | Cost: 0.1672\n",
      "epoch 10, skip_num 875, loss 0.3559, train acc 87.748%, test acc 87.640%\n",
      "EPOCH:  11 learning rate:  0.01\n",
      "Epoch: 011/030 | Batch 0100/0500 | Cost: 0.1244\n",
      "Epoch: 011/030 | Batch 0200/0500 | Cost: 0.3642\n",
      "Epoch: 011/030 | Batch 0300/0500 | Cost: 0.2652\n",
      "Epoch: 011/030 | Batch 0400/0500 | Cost: 0.3174\n",
      "Epoch: 011/030 | Batch 0500/0500 | Cost: 0.4143\n",
      "epoch 11, skip_num 1044, loss 0.3293, train acc 88.750%, test acc 87.910%\n",
      "EPOCH:  12 learning rate:  0.01\n",
      "Epoch: 012/030 | Batch 0100/0500 | Cost: 0.3275\n",
      "Epoch: 012/030 | Batch 0200/0500 | Cost: 0.1610\n",
      "Epoch: 012/030 | Batch 0300/0500 | Cost: 0.6859\n",
      "Epoch: 012/030 | Batch 0400/0500 | Cost: 1.1517\n",
      "Epoch: 012/030 | Batch 0500/0500 | Cost: 0.6680\n",
      "epoch 12, skip_num 1127, loss 0.3074, train acc 89.462%, test acc 88.140%\n",
      "EPOCH:  13 learning rate:  0.01\n",
      "Epoch: 013/030 | Batch 0100/0500 | Cost: 0.3029\n",
      "Epoch: 013/030 | Batch 0200/0500 | Cost: 0.1447\n",
      "Epoch: 013/030 | Batch 0300/0500 | Cost: 0.3771\n",
      "Epoch: 013/030 | Batch 0400/0500 | Cost: 0.1606\n",
      "Epoch: 013/030 | Batch 0500/0500 | Cost: 0.0573\n",
      "epoch 13, skip_num 1214, loss 0.2933, train acc 90.050%, test acc 88.450%\n",
      "EPOCH:  14 learning rate:  0.01\n",
      "Epoch: 014/030 | Batch 0100/0500 | Cost: 0.2482\n",
      "Epoch: 014/030 | Batch 0200/0500 | Cost: 0.2406\n",
      "Epoch: 014/030 | Batch 0300/0500 | Cost: 0.3255\n",
      "Epoch: 014/030 | Batch 0400/0500 | Cost: 0.7829\n",
      "Epoch: 014/030 | Batch 0500/0500 | Cost: 0.0220\n",
      "epoch 14, skip_num 1309, loss 0.2735, train acc 90.552%, test acc 88.340%\n",
      "EPOCH:  15 learning rate:  0.01\n",
      "Epoch: 015/030 | Batch 0100/0500 | Cost: 0.0576\n",
      "Epoch: 015/030 | Batch 0200/0500 | Cost: 0.1994\n",
      "Epoch: 015/030 | Batch 0300/0500 | Cost: 0.2276\n",
      "Epoch: 015/030 | Batch 0400/0500 | Cost: 0.1149\n",
      "Epoch: 015/030 | Batch 0500/0500 | Cost: 0.8779\n",
      "epoch 15, skip_num 1420, loss 0.2621, train acc 91.046%, test acc 88.970%\n",
      "EPOCH:  16 learning rate:  0.01\n",
      "Epoch: 016/030 | Batch 0100/0500 | Cost: 0.1411\n",
      "Epoch: 016/030 | Batch 0200/0500 | Cost: 0.1661\n",
      "Epoch: 016/030 | Batch 0300/0500 | Cost: 0.2276\n",
      "Epoch: 016/030 | Batch 0400/0500 | Cost: 0.2586\n",
      "Epoch: 016/030 | Batch 0500/0500 | Cost: 0.2277\n",
      "epoch 16, skip_num 1496, loss 0.2452, train acc 91.460%, test acc 89.020%\n",
      "EPOCH:  17 learning rate:  0.01\n",
      "Epoch: 017/030 | Batch 0100/0500 | Cost: 0.2886\n",
      "Epoch: 017/030 | Batch 0200/0500 | Cost: 0.0916\n",
      "Epoch: 017/030 | Batch 0300/0500 | Cost: 0.4854\n",
      "Epoch: 017/030 | Batch 0400/0500 | Cost: 0.0541\n",
      "Epoch: 017/030 | Batch 0500/0500 | Cost: 0.0929\n",
      "epoch 17, skip_num 1538, loss 0.2370, train acc 91.750%, test acc 89.240%\n",
      "EPOCH:  18 learning rate:  0.01\n",
      "Epoch: 018/030 | Batch 0100/0500 | Cost: 0.0831\n",
      "Epoch: 018/030 | Batch 0200/0500 | Cost: 0.1587\n",
      "Epoch: 018/030 | Batch 0300/0500 | Cost: 0.3172\n",
      "Epoch: 018/030 | Batch 0400/0500 | Cost: 0.2844\n",
      "Epoch: 018/030 | Batch 0500/0500 | Cost: 0.1708\n",
      "epoch 18, skip_num 1668, loss 0.2268, train acc 92.208%, test acc 89.640%\n",
      "EPOCH:  19 learning rate:  0.01\n",
      "Epoch: 019/030 | Batch 0100/0500 | Cost: 0.0907\n",
      "Epoch: 019/030 | Batch 0200/0500 | Cost: 0.2676\n",
      "Epoch: 019/030 | Batch 0300/0500 | Cost: 0.0646\n",
      "Epoch: 019/030 | Batch 0400/0500 | Cost: 0.0535\n",
      "Epoch: 019/030 | Batch 0500/0500 | Cost: 0.0504\n",
      "epoch 19, skip_num 1698, loss 0.2172, train acc 92.542%, test acc 89.690%\n",
      "EPOCH:  20 learning rate:  0.01\n",
      "Epoch: 020/030 | Batch 0100/0500 | Cost: 0.0445\n",
      "Epoch: 020/030 | Batch 0200/0500 | Cost: 0.2283\n",
      "Epoch: 020/030 | Batch 0300/0500 | Cost: 0.6128\n",
      "Epoch: 020/030 | Batch 0400/0500 | Cost: 0.1138\n",
      "Epoch: 020/030 | Batch 0500/0500 | Cost: 0.0483\n",
      "epoch 20, skip_num 1813, loss 0.2109, train acc 92.720%, test acc 89.910%\n",
      "EPOCH:  21 learning rate:  0.001\n",
      "Epoch: 021/030 | Batch 0100/0500 | Cost: 0.2133\n",
      "Epoch: 021/030 | Batch 0200/0500 | Cost: 0.0631\n",
      "Epoch: 021/030 | Batch 0300/0500 | Cost: 0.0847\n",
      "Epoch: 021/030 | Batch 0400/0500 | Cost: 0.1605\n",
      "Epoch: 021/030 | Batch 0500/0500 | Cost: 0.3392\n",
      "epoch 21, skip_num 1353, loss 0.1611, train acc 94.644%, test acc 91.510%\n",
      "EPOCH:  22 learning rate:  0.001\n",
      "Epoch: 022/030 | Batch 0100/0500 | Cost: 0.0479\n",
      "Epoch: 022/030 | Batch 0200/0500 | Cost: 0.1602\n",
      "Epoch: 022/030 | Batch 0300/0500 | Cost: 0.1085\n",
      "Epoch: 022/030 | Batch 0400/0500 | Cost: 0.0106\n",
      "Epoch: 022/030 | Batch 0500/0500 | Cost: 0.1024\n",
      "epoch 22, skip_num 2936, loss 0.1262, train acc 95.918%, test acc 91.670%\n",
      "EPOCH:  23 learning rate:  0.001\n",
      "Epoch: 023/030 | Batch 0100/0500 | Cost: 0.1674\n",
      "Epoch: 023/030 | Batch 0200/0500 | Cost: 0.0898\n",
      "Epoch: 023/030 | Batch 0300/0500 | Cost: 0.1347\n",
      "Epoch: 023/030 | Batch 0400/0500 | Cost: 0.1004\n",
      "Epoch: 023/030 | Batch 0500/0500 | Cost: 0.0967\n",
      "epoch 23, skip_num 2968, loss 0.1159, train acc 96.334%, test acc 91.740%\n",
      "EPOCH:  24 learning rate:  0.001\n",
      "Epoch: 024/030 | Batch 0100/0500 | Cost: 0.0874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 024/030 | Batch 0200/0500 | Cost: 0.3654\n",
      "Epoch: 024/030 | Batch 0300/0500 | Cost: 0.0208\n",
      "Epoch: 024/030 | Batch 0400/0500 | Cost: 0.0180\n",
      "Epoch: 024/030 | Batch 0500/0500 | Cost: 0.0598\n",
      "epoch 24, skip_num 3001, loss 0.1084, train acc 96.568%, test acc 91.680%\n",
      "EPOCH:  25 learning rate:  0.001\n",
      "Epoch: 025/030 | Batch 0100/0500 | Cost: 0.4215\n",
      "Epoch: 025/030 | Batch 0200/0500 | Cost: 0.2252\n",
      "Epoch: 025/030 | Batch 0300/0500 | Cost: 0.0138\n",
      "Epoch: 025/030 | Batch 0400/0500 | Cost: 0.0388\n",
      "Epoch: 025/030 | Batch 0500/0500 | Cost: 0.0243\n",
      "****************************************************************************************************\n",
      "Iter_num: 12500 Test_acc 92.01 Skip_round: 31270 Comm_round: 93730\n",
      "****************************************************************************************************\n",
      "epoch 25, skip_num 3009, loss 0.1020, train acc 96.802%, test acc 92.010%\n",
      "EPOCH:  26 learning rate:  0.001\n",
      "Epoch: 026/030 | Batch 0100/0500 | Cost: 0.0331\n",
      "Epoch: 026/030 | Batch 0200/0500 | Cost: 0.3433\n",
      "Epoch: 026/030 | Batch 0300/0500 | Cost: 0.0343\n",
      "Epoch: 026/030 | Batch 0400/0500 | Cost: 0.3548\n",
      "Epoch: 026/030 | Batch 0500/0500 | Cost: 0.0059\n",
      "epoch 26, skip_num 3010, loss 0.0983, train acc 96.858%, test acc 92.170%\n",
      "EPOCH:  27 learning rate:  0.001\n",
      "Epoch: 027/030 | Batch 0100/0500 | Cost: 0.0173\n",
      "Epoch: 027/030 | Batch 0200/0500 | Cost: 0.0355\n",
      "Epoch: 027/030 | Batch 0300/0500 | Cost: 0.3476\n",
      "Epoch: 027/030 | Batch 0400/0500 | Cost: 0.0167\n",
      "Epoch: 027/030 | Batch 0500/0500 | Cost: 0.0607\n",
      "epoch 27, skip_num 2945, loss 0.0923, train acc 97.140%, test acc 91.890%\n",
      "EPOCH:  28 learning rate:  0.001\n",
      "Epoch: 028/030 | Batch 0100/0500 | Cost: 0.0562\n",
      "Epoch: 028/030 | Batch 0200/0500 | Cost: 0.0375\n",
      "Epoch: 028/030 | Batch 0300/0500 | Cost: 0.0148\n",
      "Epoch: 028/030 | Batch 0400/0500 | Cost: 0.1155\n",
      "Epoch: 028/030 | Batch 0500/0500 | Cost: 0.0916\n",
      "epoch 28, skip_num 3041, loss 0.0913, train acc 97.150%, test acc 92.150%\n",
      "EPOCH:  29 learning rate:  0.001\n",
      "Epoch: 029/030 | Batch 0100/0500 | Cost: 0.2560\n",
      "Epoch: 029/030 | Batch 0200/0500 | Cost: 0.0269\n",
      "Epoch: 029/030 | Batch 0300/0500 | Cost: 0.0628\n",
      "Epoch: 029/030 | Batch 0400/0500 | Cost: 0.0264\n",
      "Epoch: 029/030 | Batch 0500/0500 | Cost: 0.0179\n",
      "epoch 29, skip_num 3045, loss 0.0834, train acc 97.452%, test acc 91.590%\n",
      "EPOCH:  30 learning rate:  0.001\n",
      "Epoch: 030/030 | Batch 0100/0500 | Cost: 0.1125\n",
      "Epoch: 030/030 | Batch 0200/0500 | Cost: 0.2814\n",
      "Epoch: 030/030 | Batch 0300/0500 | Cost: 0.0703\n",
      "Epoch: 030/030 | Batch 0400/0500 | Cost: 0.0161\n",
      "Epoch: 030/030 | Batch 0500/0500 | Cost: 0.0246\n",
      "epoch 30, skip_num 3029, loss 0.0822, train acc 97.434%, test acc 91.930%\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "print(\"SASG  lr:\"+str(lr)+\"--h:\"+str(h)+\"--epoch:\"+str(NUM_EPOCHS)+\"--worker:\"+str(NUM_WORKERS))\n",
    "seed_torch(RANDOM_SEED)\n",
    "model = resnet18()\n",
    "model.to(DEVICE)\n",
    "model_copy = resnet18_copy()\n",
    "model_copy.to(DEVICE)\n",
    "loss = nn.CrossEntropyLoss()  # 损失函数\n",
    "\n",
    "# prepare data\n",
    "dataset_name = 'cifar10'\n",
    "train_loader_workers, test_loader = create_loaders(dataset_name, NUM_WORKERS, BATCH_SIZE)\n",
    "\n",
    "NUM_PARAS = sum([1 for param in model.parameters()])\n",
    "print(\"Number_parameter:\", NUM_PARAS)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Element_parameter:\", total_params)\n",
    "total_params_sparse = 0\n",
    "for p in model.parameters():\n",
    "    x, dim, d = prep_grad(p)\n",
    "    r = int(np.maximum(1, np.floor(d * h)))\n",
    "    total_params_sparse += r\n",
    "print(\"Element_parameter_sparse:\", total_params_sparse)\n",
    "\n",
    "state_dict = [0 for col in range(NUM_WORKERS)]\n",
    "grad_worker = [[0 for col in range(NUM_PARAS)] for row in range(NUM_WORKERS)]\n",
    "error_worker = [[0 for col in range(NUM_PARAS)] for row in range(NUM_WORKERS)]\n",
    "grad_old = [0 for col in range(NUM_PARAS)]\n",
    "grad_new = [0 for col in range(NUM_PARAS)]\n",
    "para_store = [0 for col in range(NUM_PARAS)]\n",
    "tau = [0 for col in range(NUM_WORKERS)]\n",
    "para_list = []\n",
    "Skip_epoch = []\n",
    "Loss_epoch = []\n",
    "train_acc_epoch = []\n",
    "test_acc_epoch = []\n",
    "Skip_iter = []\n",
    "Loss_iter = []\n",
    "train_acc_iter = []\n",
    "test_acc_iter = []\n",
    "\n",
    "iter_num = 0\n",
    "skip_iter = 0\n",
    "flag_acc = False\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_l_sum, train_acc_sum, num = 0.0, 0.0, 0\n",
    "    train_loader_iter = [iter(train_loader_workers[w]) for w in range(NUM_WORKERS)]\n",
    "    iter_steps = len(train_loader_workers[0])\n",
    "    skip_num = [[0 for col in range(iter_steps)] for row in range(NUM_EPOCHS)]\n",
    "    \n",
    "    # adjust learning_rate\n",
    "    lr, lr_thead = adjust_lr(epoch)\n",
    "    print(\"EPOCH: \", epoch + 1, \"learning rate: \", lr)\n",
    "    \n",
    "    for batch_idx in range(iter_steps):\n",
    "        model.train()\n",
    "        model_copy.train()\n",
    "        grad_agg = [0 for col in range(NUM_PARAS)]\n",
    "        skip_idx = 0\n",
    "        if (epoch * iter_steps + batch_idx) < D:\n",
    "            for w_id in range(NUM_WORKERS):\n",
    "                images, labels = next(train_loader_iter[w_id])\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                y_hat = model(images)\n",
    "                Loss = loss(y_hat, labels)\n",
    "                Loss.backward()\n",
    "                with torch.no_grad():\n",
    "                    p_id = 0\n",
    "                    for p in model.parameters():\n",
    "                        g = lr * p.grad.data.clone().detach() + error_worker[w_id][p_id]\n",
    "                        Tk_sparse = top_k_opt(g, h)\n",
    "                        grad_agg[p_id] += Tk_sparse\n",
    "                        error_worker[w_id][p_id] = g - Tk_sparse\n",
    "                        grad_worker[w_id][p_id] = Tk_sparse\n",
    "                        p_id += 1\n",
    "                        p.grad.zero_()\n",
    "                state_dict[w_id] = copy.deepcopy(model.state_dict())\n",
    "                train_l_sum += Loss.item()\n",
    "                train_acc_sum += (y_hat.argmax(dim=1) == labels).sum().item()\n",
    "                num += labels.shape[0]\n",
    "        else:\n",
    "            thread = 0.0\n",
    "            with torch.no_grad():\n",
    "                for i in range(len(para_list) - 1):\n",
    "                    thread += compt(para_list[i], para_list[i + 1])\n",
    "            thread = thread / (lr_thead * (NUM_WORKERS ** 2))\n",
    "            for w_id in range(NUM_WORKERS):\n",
    "                images, labels = next(train_loader_iter[w_id])\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                model_copy.load_state_dict(state_dict[w_id])\n",
    "                y_copy = model_copy(images)\n",
    "                Loss_copy = loss(y_copy, labels)\n",
    "                Loss_copy.backward()\n",
    "                y_hat = model(images)\n",
    "                Loss = loss(y_hat, labels)\n",
    "                Loss.backward()\n",
    "                with torch.no_grad():\n",
    "                    p_old_id = 0\n",
    "                    for p_old in model_copy.parameters():\n",
    "                        grad_old[p_old_id] = p_old.grad.data.clone().detach()\n",
    "                        p_old_id += 1\n",
    "                        p_old.grad.zero_()\n",
    "                    p_id = 0\n",
    "                    for p in model.parameters():\n",
    "                        grad_new[p_id] = p.grad.data.clone().detach()\n",
    "                        p_id += 1\n",
    "                    if compt(grad_old, grad_new) > thread or tau[w_id] > D:\n",
    "                        tau[w_id] = 1\n",
    "                        p_id = 0\n",
    "                        for p in model.parameters():\n",
    "                            g = lr * p.grad.data.clone().detach() + error_worker[w_id][p_id]\n",
    "                            Tk_sparse = top_k_opt(g, h)\n",
    "                            grad_agg[p_id] += Tk_sparse\n",
    "                            error_worker[w_id][p_id] = g - Tk_sparse\n",
    "                            grad_worker[w_id][p_id] = Tk_sparse\n",
    "                            p_id += 1\n",
    "                            p.grad.zero_()\n",
    "                        state_dict[w_id] = copy.deepcopy(model.state_dict())\n",
    "                    else:\n",
    "                        skip_idx += 1\n",
    "                        tau[w_id] += 1\n",
    "                        p_id = 0\n",
    "                        for p in model.parameters():\n",
    "                            grad_agg[p_id] += grad_worker[w_id][p_id]\n",
    "                            p_id += 1\n",
    "                            p.grad.zero_()\n",
    "                train_l_sum += Loss.item()\n",
    "                train_acc_sum += (y_hat.argmax(dim=1) == labels).sum().item()\n",
    "                num += labels.shape[0]\n",
    "        skip_num[epoch][batch_idx] = skip_idx\n",
    "        with torch.no_grad():\n",
    "            p_id = 0\n",
    "            for p in model.parameters():\n",
    "                p.data.add_(grad_agg[p_id], alpha=-1)\n",
    "                para_store[p_id] = p.data.clone().detach()\n",
    "                p_id += 1\n",
    "                p.grad.zero_()\n",
    "            para_list.insert(0, para_store.copy())\n",
    "            if len(para_list) > D:\n",
    "                para_list.pop()\n",
    "\n",
    "        # LOGGING\n",
    "        if not (batch_idx + 1) % 100:\n",
    "            print('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f'\n",
    "                  % (epoch + 1, NUM_EPOCHS, batch_idx + 1, len(train_loader_workers[0]), Loss.item()))\n",
    "        iter_num += 1\n",
    "        skip_iter += skip_idx\n",
    "        if not iter_num % 100:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_acc_it = evaluate_accuracy(model, test_loader, DEVICE)\n",
    "                Skip_iter.append(skip_iter)\n",
    "                Loss_iter.append(train_l_sum / (num / BATCH_SIZE))\n",
    "                train_acc_iter.append(train_acc_sum / num * 100)\n",
    "                test_acc_iter.append(test_acc_it)\n",
    "            if test_acc_it > 92.0 and not flag_acc:\n",
    "                flag_acc = True\n",
    "                print(\"*\" * 100)\n",
    "                print(\"Iter_num:\", iter_num, \"Test_acc\", test_acc_it, \"Skip_round:\", skip_iter, \"Comm_round:\", iter_num * 10 - skip_iter)  # 10 workers\n",
    "                print(\"*\" * 100)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_acc = evaluate_accuracy(model, test_loader, DEVICE)\n",
    "        Skip_epoch.append(sum(skip_num[epoch]))\n",
    "        Loss_epoch.append(train_l_sum / (num / BATCH_SIZE))\n",
    "        train_acc_epoch.append(train_acc_sum / num * 100)\n",
    "        test_acc_epoch.append(test_acc)\n",
    "    print('epoch %d, skip_num %d, loss %.4f, train acc %.3f%%, test acc %.3f%%'\n",
    "          % (epoch + 1, Skip_epoch[epoch], train_l_sum / (num / BATCH_SIZE), train_acc_sum / num * 100, test_acc))\n",
    "          \n",
    "print('Finished.')\n",
    "\n",
    "list_write = []\n",
    "list_write.append(Skip_epoch)\n",
    "list_write.append(Loss_epoch)\n",
    "list_write.append(train_acc_epoch)\n",
    "list_write.append(test_acc_epoch)\n",
    "name = ['Skip', 'Loss', 'train-acc', 'test-acc']\n",
    "test = pd.DataFrame(index=name, data=list_write).T\n",
    "test.to_csv(\"./result/\"+\"SASG-res-cifar10.csv\", encoding='gbk')\n",
    "\n",
    "list_write_iter = []\n",
    "list_write_iter.append(Skip_iter)\n",
    "list_write_iter.append(Loss_iter)\n",
    "list_write_iter.append(train_acc_iter)\n",
    "list_write_iter.append(test_acc_iter)\n",
    "name_iter = ['Skip', 'Loss', 'train-acc', 'test-acc']\n",
    "test = pd.DataFrame(index=name_iter, data=list_write_iter).T\n",
    "test.to_csv(\"./result/\"+\"SASG-res-cifar10-iter.csv\", encoding='gbk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LASG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LASG  lr:0.001--epoch:30--worker:10\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Number_parameter: 62\n",
      "Element_parameter: 11173962\n",
      "EPOCH:  1 learning rate:  0.01\n",
      "Epoch: 001/030 | Batch 0100/0500 | Cost: 1.7659\n",
      "Epoch: 001/030 | Batch 0200/0500 | Cost: 1.4933\n",
      "Epoch: 001/030 | Batch 0300/0500 | Cost: 1.6559\n",
      "Epoch: 001/030 | Batch 0400/0500 | Cost: 1.8860\n",
      "Epoch: 001/030 | Batch 0500/0500 | Cost: 1.6803\n",
      "epoch 1, skip_num 0, loss 1.7549, train acc 36.730%, test acc 43.470%\n",
      "EPOCH:  2 learning rate:  0.01\n",
      "Epoch: 002/030 | Batch 0100/0500 | Cost: 1.9076\n",
      "Epoch: 002/030 | Batch 0200/0500 | Cost: 1.3421\n",
      "Epoch: 002/030 | Batch 0300/0500 | Cost: 1.5174\n",
      "Epoch: 002/030 | Batch 0400/0500 | Cost: 0.7951\n",
      "Epoch: 002/030 | Batch 0500/0500 | Cost: 1.6016\n",
      "epoch 2, skip_num 0, loss 1.2107, train acc 57.036%, test acc 64.660%\n",
      "EPOCH:  3 learning rate:  0.01\n",
      "Epoch: 003/030 | Batch 0100/0500 | Cost: 0.7909\n",
      "Epoch: 003/030 | Batch 0200/0500 | Cost: 1.4459\n",
      "Epoch: 003/030 | Batch 0300/0500 | Cost: 0.8927\n",
      "Epoch: 003/030 | Batch 0400/0500 | Cost: 0.7410\n",
      "Epoch: 003/030 | Batch 0500/0500 | Cost: 1.2323\n",
      "epoch 3, skip_num 0, loss 0.9360, train acc 67.418%, test acc 75.710%\n",
      "EPOCH:  4 learning rate:  0.01\n",
      "Epoch: 004/030 | Batch 0100/0500 | Cost: 0.5863\n",
      "Epoch: 004/030 | Batch 0200/0500 | Cost: 0.7857\n",
      "Epoch: 004/030 | Batch 0300/0500 | Cost: 1.2190\n",
      "Epoch: 004/030 | Batch 0400/0500 | Cost: 0.7880\n",
      "Epoch: 004/030 | Batch 0500/0500 | Cost: 1.1280\n",
      "epoch 4, skip_num 1, loss 0.7640, train acc 73.480%, test acc 78.190%\n",
      "EPOCH:  5 learning rate:  0.01\n",
      "Epoch: 005/030 | Batch 0100/0500 | Cost: 0.7100\n",
      "Epoch: 005/030 | Batch 0200/0500 | Cost: 0.6807\n",
      "Epoch: 005/030 | Batch 0300/0500 | Cost: 0.3402\n",
      "Epoch: 005/030 | Batch 0400/0500 | Cost: 0.4234\n",
      "Epoch: 005/030 | Batch 0500/0500 | Cost: 1.1742\n",
      "epoch 5, skip_num 4, loss 0.6607, train acc 77.366%, test acc 77.740%\n",
      "EPOCH:  6 learning rate:  0.01\n",
      "Epoch: 006/030 | Batch 0100/0500 | Cost: 0.9101\n",
      "Epoch: 006/030 | Batch 0200/0500 | Cost: 0.3088\n",
      "Epoch: 006/030 | Batch 0300/0500 | Cost: 1.0978\n",
      "Epoch: 006/030 | Batch 0400/0500 | Cost: 0.7872\n",
      "Epoch: 006/030 | Batch 0500/0500 | Cost: 0.3722\n",
      "epoch 6, skip_num 6, loss 0.5825, train acc 79.928%, test acc 83.450%\n",
      "EPOCH:  7 learning rate:  0.01\n",
      "Epoch: 007/030 | Batch 0100/0500 | Cost: 0.1625\n",
      "Epoch: 007/030 | Batch 0200/0500 | Cost: 0.5763\n",
      "Epoch: 007/030 | Batch 0300/0500 | Cost: 0.1861\n",
      "Epoch: 007/030 | Batch 0400/0500 | Cost: 0.2970\n",
      "Epoch: 007/030 | Batch 0500/0500 | Cost: 0.9354\n",
      "epoch 7, skip_num 11, loss 0.5177, train acc 82.232%, test acc 82.930%\n",
      "EPOCH:  8 learning rate:  0.01\n",
      "Epoch: 008/030 | Batch 0100/0500 | Cost: 0.7927\n",
      "Epoch: 008/030 | Batch 0200/0500 | Cost: 0.4829\n",
      "Epoch: 008/030 | Batch 0300/0500 | Cost: 0.6330\n",
      "Epoch: 008/030 | Batch 0400/0500 | Cost: 0.5830\n",
      "Epoch: 008/030 | Batch 0500/0500 | Cost: 0.2351\n",
      "epoch 8, skip_num 22, loss 0.4732, train acc 83.664%, test acc 82.820%\n",
      "EPOCH:  9 learning rate:  0.01\n",
      "Epoch: 009/030 | Batch 0100/0500 | Cost: 0.6036\n",
      "Epoch: 009/030 | Batch 0200/0500 | Cost: 0.4155\n",
      "Epoch: 009/030 | Batch 0300/0500 | Cost: 0.3624\n",
      "Epoch: 009/030 | Batch 0400/0500 | Cost: 0.4548\n",
      "Epoch: 009/030 | Batch 0500/0500 | Cost: 0.2131\n",
      "epoch 9, skip_num 42, loss 0.4343, train acc 84.966%, test acc 84.680%\n",
      "EPOCH:  10 learning rate:  0.01\n",
      "Epoch: 010/030 | Batch 0100/0500 | Cost: 0.0794\n",
      "Epoch: 010/030 | Batch 0200/0500 | Cost: 0.4046\n",
      "Epoch: 010/030 | Batch 0300/0500 | Cost: 0.3974\n",
      "Epoch: 010/030 | Batch 0400/0500 | Cost: 0.1858\n",
      "Epoch: 010/030 | Batch 0500/0500 | Cost: 0.2249\n",
      "epoch 10, skip_num 70, loss 0.4029, train acc 86.344%, test acc 86.080%\n",
      "EPOCH:  11 learning rate:  0.01\n",
      "Epoch: 011/030 | Batch 0100/0500 | Cost: 0.3899\n",
      "Epoch: 011/030 | Batch 0200/0500 | Cost: 0.6382\n",
      "Epoch: 011/030 | Batch 0300/0500 | Cost: 0.1638\n",
      "Epoch: 011/030 | Batch 0400/0500 | Cost: 0.4771\n",
      "Epoch: 011/030 | Batch 0500/0500 | Cost: 0.6334\n",
      "epoch 11, skip_num 109, loss 0.3764, train acc 87.054%, test acc 84.880%\n",
      "EPOCH:  12 learning rate:  0.01\n",
      "Epoch: 012/030 | Batch 0100/0500 | Cost: 0.8858\n",
      "Epoch: 012/030 | Batch 0200/0500 | Cost: 0.4100\n",
      "Epoch: 012/030 | Batch 0300/0500 | Cost: 0.7480\n",
      "Epoch: 012/030 | Batch 0400/0500 | Cost: 0.8249\n",
      "Epoch: 012/030 | Batch 0500/0500 | Cost: 0.2916\n",
      "epoch 12, skip_num 121, loss 0.3495, train acc 88.072%, test acc 85.200%\n",
      "EPOCH:  13 learning rate:  0.01\n",
      "Epoch: 013/030 | Batch 0100/0500 | Cost: 0.2241\n",
      "Epoch: 013/030 | Batch 0200/0500 | Cost: 0.1916\n",
      "Epoch: 013/030 | Batch 0300/0500 | Cost: 0.5193\n",
      "Epoch: 013/030 | Batch 0400/0500 | Cost: 0.2771\n",
      "Epoch: 013/030 | Batch 0500/0500 | Cost: 0.0837\n",
      "epoch 13, skip_num 154, loss 0.3318, train acc 88.630%, test acc 87.460%\n",
      "EPOCH:  14 learning rate:  0.01\n",
      "Epoch: 014/030 | Batch 0100/0500 | Cost: 0.0883\n",
      "Epoch: 014/030 | Batch 0200/0500 | Cost: 0.4378\n",
      "Epoch: 014/030 | Batch 0300/0500 | Cost: 0.3470\n",
      "Epoch: 014/030 | Batch 0400/0500 | Cost: 0.1613\n",
      "Epoch: 014/030 | Batch 0500/0500 | Cost: 0.0441\n",
      "epoch 14, skip_num 171, loss 0.3047, train acc 89.454%, test acc 88.100%\n",
      "EPOCH:  15 learning rate:  0.01\n",
      "Epoch: 015/030 | Batch 0100/0500 | Cost: 0.1923\n",
      "Epoch: 015/030 | Batch 0200/0500 | Cost: 0.3209\n",
      "Epoch: 015/030 | Batch 0300/0500 | Cost: 0.0582\n",
      "Epoch: 015/030 | Batch 0400/0500 | Cost: 0.2323\n",
      "Epoch: 015/030 | Batch 0500/0500 | Cost: 0.8206\n",
      "epoch 15, skip_num 225, loss 0.2868, train acc 90.032%, test acc 87.930%\n",
      "EPOCH:  16 learning rate:  0.01\n",
      "Epoch: 016/030 | Batch 0100/0500 | Cost: 0.1142\n",
      "Epoch: 016/030 | Batch 0200/0500 | Cost: 0.1758\n",
      "Epoch: 016/030 | Batch 0300/0500 | Cost: 0.3856\n",
      "Epoch: 016/030 | Batch 0400/0500 | Cost: 0.4770\n",
      "Epoch: 016/030 | Batch 0500/0500 | Cost: 0.3069\n",
      "epoch 16, skip_num 245, loss 0.2724, train acc 90.520%, test acc 88.120%\n",
      "EPOCH:  17 learning rate:  0.01\n",
      "Epoch: 017/030 | Batch 0100/0500 | Cost: 0.3293\n",
      "Epoch: 017/030 | Batch 0200/0500 | Cost: 0.1098\n",
      "Epoch: 017/030 | Batch 0300/0500 | Cost: 0.5293\n",
      "Epoch: 017/030 | Batch 0400/0500 | Cost: 0.2975\n",
      "Epoch: 017/030 | Batch 0500/0500 | Cost: 0.1453\n",
      "epoch 17, skip_num 300, loss 0.2606, train acc 91.074%, test acc 87.020%\n",
      "EPOCH:  18 learning rate:  0.01\n",
      "Epoch: 018/030 | Batch 0100/0500 | Cost: 0.2342\n",
      "Epoch: 018/030 | Batch 0200/0500 | Cost: 0.4476\n",
      "Epoch: 018/030 | Batch 0300/0500 | Cost: 0.6466\n",
      "Epoch: 018/030 | Batch 0400/0500 | Cost: 0.3888\n",
      "Epoch: 018/030 | Batch 0500/0500 | Cost: 0.1638\n",
      "epoch 18, skip_num 318, loss 0.2478, train acc 91.432%, test acc 89.820%\n",
      "EPOCH:  19 learning rate:  0.01\n",
      "Epoch: 019/030 | Batch 0100/0500 | Cost: 0.1146\n",
      "Epoch: 019/030 | Batch 0200/0500 | Cost: 0.1813\n",
      "Epoch: 019/030 | Batch 0300/0500 | Cost: 0.1729\n",
      "Epoch: 019/030 | Batch 0400/0500 | Cost: 0.0724\n",
      "Epoch: 019/030 | Batch 0500/0500 | Cost: 0.0781\n",
      "epoch 19, skip_num 348, loss 0.2343, train acc 91.718%, test acc 88.450%\n",
      "EPOCH:  20 learning rate:  0.01\n",
      "Epoch: 020/030 | Batch 0100/0500 | Cost: 0.3021\n",
      "Epoch: 020/030 | Batch 0200/0500 | Cost: 0.2550\n",
      "Epoch: 020/030 | Batch 0300/0500 | Cost: 0.8567\n",
      "Epoch: 020/030 | Batch 0400/0500 | Cost: 0.0978\n",
      "Epoch: 020/030 | Batch 0500/0500 | Cost: 0.0468\n",
      "epoch 20, skip_num 417, loss 0.2249, train acc 92.292%, test acc 85.180%\n",
      "EPOCH:  21 learning rate:  0.001\n",
      "Epoch: 021/030 | Batch 0100/0500 | Cost: 0.1417\n",
      "Epoch: 021/030 | Batch 0200/0500 | Cost: 0.2057\n",
      "Epoch: 021/030 | Batch 0300/0500 | Cost: 0.0080\n",
      "Epoch: 021/030 | Batch 0400/0500 | Cost: 0.3200\n",
      "Epoch: 021/030 | Batch 0500/0500 | Cost: 0.0700\n",
      "epoch 21, skip_num 446, loss 0.1491, train acc 94.970%, test acc 91.690%\n",
      "EPOCH:  22 learning rate:  0.001\n",
      "Epoch: 022/030 | Batch 0100/0500 | Cost: 0.0431\n",
      "Epoch: 022/030 | Batch 0200/0500 | Cost: 0.1605\n",
      "Epoch: 022/030 | Batch 0300/0500 | Cost: 0.2303\n",
      "Epoch: 022/030 | Batch 0400/0500 | Cost: 0.0130\n",
      "Epoch: 022/030 | Batch 0500/0500 | Cost: 0.0837\n",
      "epoch 22, skip_num 1497, loss 0.1298, train acc 95.608%, test acc 91.910%\n",
      "EPOCH:  23 learning rate:  0.001\n",
      "Epoch: 023/030 | Batch 0100/0500 | Cost: 0.0636\n",
      "Epoch: 023/030 | Batch 0200/0500 | Cost: 0.0488\n",
      "Epoch: 023/030 | Batch 0300/0500 | Cost: 0.1411\n",
      "Epoch: 023/030 | Batch 0400/0500 | Cost: 0.0912\n",
      "Epoch: 023/030 | Batch 0500/0500 | Cost: 0.1264\n",
      "epoch 23, skip_num 1546, loss 0.1223, train acc 95.932%, test acc 91.820%\n",
      "EPOCH:  24 learning rate:  0.001\n",
      "Epoch: 024/030 | Batch 0100/0500 | Cost: 0.1835\n",
      "Epoch: 024/030 | Batch 0200/0500 | Cost: 0.4145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 024/030 | Batch 0300/0500 | Cost: 0.0598\n",
      "Epoch: 024/030 | Batch 0400/0500 | Cost: 0.0510\n",
      "****************************************************************************************************\n",
      "Iter_num: 11900 Test_acc 92.05 Skip_round: 7313 Comm_round: 111687\n",
      "****************************************************************************************************\n",
      "Epoch: 024/030 | Batch 0500/0500 | Cost: 0.0271\n",
      "epoch 24, skip_num 1560, loss 0.1175, train acc 96.100%, test acc 92.040%\n",
      "EPOCH:  25 learning rate:  0.001\n",
      "Epoch: 025/030 | Batch 0100/0500 | Cost: 0.4641\n",
      "Epoch: 025/030 | Batch 0200/0500 | Cost: 0.3206\n",
      "Epoch: 025/030 | Batch 0300/0500 | Cost: 0.0292\n",
      "Epoch: 025/030 | Batch 0400/0500 | Cost: 0.1273\n",
      "Epoch: 025/030 | Batch 0500/0500 | Cost: 0.0104\n",
      "epoch 25, skip_num 1618, loss 0.1104, train acc 96.380%, test acc 91.820%\n",
      "EPOCH:  26 learning rate:  0.001\n",
      "Epoch: 026/030 | Batch 0100/0500 | Cost: 0.0374\n",
      "Epoch: 026/030 | Batch 0200/0500 | Cost: 0.1940\n",
      "Epoch: 026/030 | Batch 0300/0500 | Cost: 0.0365\n",
      "Epoch: 026/030 | Batch 0400/0500 | Cost: 0.1984\n",
      "Epoch: 026/030 | Batch 0500/0500 | Cost: 0.0060\n",
      "epoch 26, skip_num 1626, loss 0.1072, train acc 96.504%, test acc 92.040%\n",
      "EPOCH:  27 learning rate:  0.001\n",
      "Epoch: 027/030 | Batch 0100/0500 | Cost: 0.0332\n",
      "Epoch: 027/030 | Batch 0200/0500 | Cost: 0.0168\n",
      "Epoch: 027/030 | Batch 0300/0500 | Cost: 0.1820\n",
      "Epoch: 027/030 | Batch 0400/0500 | Cost: 0.0120\n",
      "Epoch: 027/030 | Batch 0500/0500 | Cost: 0.1642\n",
      "epoch 27, skip_num 1637, loss 0.1006, train acc 96.666%, test acc 91.790%\n",
      "EPOCH:  28 learning rate:  0.001\n",
      "Epoch: 028/030 | Batch 0100/0500 | Cost: 0.0275\n",
      "Epoch: 028/030 | Batch 0200/0500 | Cost: 0.0180\n",
      "Epoch: 028/030 | Batch 0300/0500 | Cost: 0.0400\n",
      "Epoch: 028/030 | Batch 0400/0500 | Cost: 0.3055\n",
      "Epoch: 028/030 | Batch 0500/0500 | Cost: 0.0466\n",
      "epoch 28, skip_num 1702, loss 0.1010, train acc 96.658%, test acc 91.990%\n",
      "EPOCH:  29 learning rate:  0.001\n",
      "Epoch: 029/030 | Batch 0100/0500 | Cost: 0.0551\n",
      "Epoch: 029/030 | Batch 0200/0500 | Cost: 0.0205\n",
      "Epoch: 029/030 | Batch 0300/0500 | Cost: 0.0446\n",
      "Epoch: 029/030 | Batch 0400/0500 | Cost: 0.0167\n",
      "Epoch: 029/030 | Batch 0500/0500 | Cost: 0.0221\n",
      "epoch 29, skip_num 1653, loss 0.0962, train acc 96.784%, test acc 91.980%\n",
      "EPOCH:  30 learning rate:  0.001\n",
      "Epoch: 030/030 | Batch 0100/0500 | Cost: 0.0465\n",
      "Epoch: 030/030 | Batch 0200/0500 | Cost: 0.1680\n",
      "Epoch: 030/030 | Batch 0300/0500 | Cost: 0.0451\n",
      "Epoch: 030/030 | Batch 0400/0500 | Cost: 0.0071\n",
      "Epoch: 030/030 | Batch 0500/0500 | Cost: 0.0137\n",
      "epoch 30, skip_num 1708, loss 0.0921, train acc 97.050%, test acc 92.230%\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "print(\"LASG  lr:\"+str(lr)+\"--epoch:\"+str(NUM_EPOCHS)+\"--worker:\"+str(NUM_WORKERS))\n",
    "seed_torch(RANDOM_SEED)\n",
    "model = resnet18()\n",
    "model.to(DEVICE)\n",
    "model_copy = resnet18_copy()\n",
    "model_copy.to(DEVICE)\n",
    "loss = nn.CrossEntropyLoss()  # 损失函数\n",
    "\n",
    "# prepare data\n",
    "dataset_name = 'cifar10'\n",
    "train_loader_workers, test_loader = create_loaders(dataset_name, NUM_WORKERS, BATCH_SIZE)\n",
    "\n",
    "state_dict = [0 for col in range(NUM_WORKERS)]\n",
    "NUM_PARAS = sum([1 for param in model.parameters()])\n",
    "print(\"Number_parameter:\", NUM_PARAS)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Element_parameter:\", total_params)\n",
    "\n",
    "grad_worker = [[0 for col in range(NUM_PARAS)] for row in range(NUM_WORKERS)]\n",
    "grad_old = [0 for col in range(NUM_PARAS)]\n",
    "grad_new = [0 for col in range(NUM_PARAS)]\n",
    "para_store = [0 for col in range(NUM_PARAS)]\n",
    "tau = [0 for col in range(NUM_WORKERS)]\n",
    "para_list = []\n",
    "Skip_epoch = []\n",
    "Loss_epoch = []\n",
    "train_acc_epoch = []\n",
    "test_acc_epoch = []\n",
    "Skip_iter = []\n",
    "Loss_iter = []\n",
    "train_acc_iter = []\n",
    "test_acc_iter = []\n",
    "\n",
    "iter_num = 0\n",
    "skip_iter = 0\n",
    "flag_acc = False\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_l_sum, train_acc_sum, num = 0.0, 0.0, 0\n",
    "    train_loader_iter = [iter(train_loader_workers[w]) for w in range(NUM_WORKERS)]\n",
    "    iter_steps = len(train_loader_workers[0])\n",
    "    skip_num = [[0 for col in range(iter_steps)] for row in range(NUM_EPOCHS)]\n",
    "    \n",
    "    # adjust learning_rate\n",
    "    lr, lr_thead = adjust_lr(epoch)\n",
    "    print(\"EPOCH: \", epoch + 1, \"learning rate: \", lr)\n",
    "\n",
    "    for batch_idx in range(iter_steps):\n",
    "        model.train()\n",
    "        model_copy.train()\n",
    "        grad_agg = [0 for col in range(NUM_PARAS)]\n",
    "        skip_idx = 0\n",
    "        if (epoch * iter_steps + batch_idx) < D:\n",
    "            for w_id in range(NUM_WORKERS):\n",
    "                images, labels = next(train_loader_iter[w_id])\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                y_hat = model(images)\n",
    "                Loss = loss(y_hat, labels)\n",
    "                Loss.backward()\n",
    "                with torch.no_grad():\n",
    "                    p_id = 0\n",
    "                    for p in model.parameters():\n",
    "                        g = lr * p.grad.data.clone().detach()\n",
    "                        grad_agg[p_id] += g\n",
    "                        grad_worker[w_id][p_id] = g\n",
    "                        p_id += 1\n",
    "                        p.grad.zero_()\n",
    "                state_dict[w_id] = copy.deepcopy(model.state_dict())\n",
    "                train_l_sum += Loss.item()\n",
    "                train_acc_sum += (y_hat.argmax(dim=1) == labels).sum().item()\n",
    "                num += labels.shape[0]\n",
    "        else:\n",
    "            thread = 0.0\n",
    "            with torch.no_grad():\n",
    "                for i in range(len(para_list) - 1):\n",
    "                    thread += compt(para_list[i], para_list[i + 1])\n",
    "            thread = thread / (lr_thead * (NUM_WORKERS ** 2))\n",
    "            for w_id in range(NUM_WORKERS):\n",
    "                images, labels = next(train_loader_iter[w_id])\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                model_copy.load_state_dict(state_dict[w_id])\n",
    "                y_copy = model_copy(images)\n",
    "                Loss_copy = loss(y_copy, labels)\n",
    "                Loss_copy.backward()\n",
    "                y_hat = model(images)\n",
    "                Loss = loss(y_hat, labels)\n",
    "                Loss.backward()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    p_old_id = 0\n",
    "                    for p_old in model_copy.parameters():\n",
    "                        grad_old[p_old_id] = p_old.grad.data.clone().detach()\n",
    "                        p_old_id += 1\n",
    "                        p_old.grad.zero_()\n",
    "                    p_id = 0\n",
    "                    for p in model.parameters():\n",
    "                        grad_new[p_id] = p.grad.data.clone().detach()\n",
    "                        p_id += 1\n",
    "                    if compt(grad_old, grad_new) > thread or tau[w_id] > D:\n",
    "                        tau[w_id] = 1\n",
    "                        p_id = 0\n",
    "                        for p in model.parameters():\n",
    "                            g = lr * p.grad.data.clone().detach()\n",
    "                            grad_agg[p_id] += g\n",
    "                            grad_worker[w_id][p_id] = g\n",
    "                            p_id += 1\n",
    "                            p.grad.zero_()\n",
    "                        state_dict[w_id] = copy.deepcopy(model.state_dict())\n",
    "                    else:\n",
    "                        skip_idx += 1\n",
    "                        tau[w_id] += 1\n",
    "                        p_id = 0\n",
    "                        for p in model.parameters():\n",
    "                            grad_agg[p_id] += grad_worker[w_id][p_id]\n",
    "                            p_id += 1\n",
    "                            p.grad.zero_()\n",
    "                train_l_sum += Loss.item()\n",
    "                train_acc_sum += (y_hat.argmax(dim=1) == labels).sum().item()\n",
    "                num += labels.shape[0]\n",
    "        skip_num[epoch][batch_idx] = skip_idx\n",
    "        with torch.no_grad():\n",
    "            p_id = 0\n",
    "            for p in model.parameters():\n",
    "                p.data.add_(grad_agg[p_id], alpha=-1)\n",
    "                para_store[p_id] = p.data.clone().detach()\n",
    "                p_id += 1\n",
    "                p.grad.zero_()\n",
    "\n",
    "            para_list.insert(0, para_store.copy())\n",
    "            if len(para_list) > D:\n",
    "                para_list.pop()\n",
    "        # LOGGING\n",
    "        if not (batch_idx + 1) % 100:\n",
    "            print('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f'\n",
    "                  % (epoch + 1, NUM_EPOCHS, batch_idx + 1, len(train_loader_workers[0]), Loss.item()))\n",
    "        iter_num += 1\n",
    "        skip_iter += skip_idx\n",
    "        if not iter_num % 100:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_acc_it = evaluate_accuracy(model, test_loader, DEVICE)\n",
    "                Skip_iter.append(skip_iter)\n",
    "                Loss_iter.append(train_l_sum / (num / BATCH_SIZE))\n",
    "                train_acc_iter.append(train_acc_sum / num * 100)\n",
    "                test_acc_iter.append(test_acc_it)\n",
    "            if test_acc_it > 92.0 and not flag_acc:\n",
    "                flag_acc = True\n",
    "                print(\"*\" * 100)\n",
    "                print(\"Iter_num:\", iter_num, \"Test_acc\", test_acc_it, \"Skip_round:\", skip_iter, \"Comm_round:\", iter_num * 10 - skip_iter)  # 10 workers\n",
    "                print(\"*\" * 100)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_acc = evaluate_accuracy(model, test_loader, DEVICE)\n",
    "        Skip_epoch.append(sum(skip_num[epoch]))\n",
    "        Loss_epoch.append(train_l_sum / (num / BATCH_SIZE))\n",
    "        train_acc_epoch.append(train_acc_sum / num * 100)\n",
    "        test_acc_epoch.append(test_acc)\n",
    "    print('epoch %d, skip_num %d, loss %.4f, train acc %.3f%%, test acc %.3f%%'\n",
    "          % (epoch + 1, Skip_epoch[epoch], train_l_sum / (num / BATCH_SIZE), train_acc_sum / num * 100, test_acc))\n",
    "\n",
    "print('Finished.')\n",
    "\n",
    "list_write = []\n",
    "list_write.append(Skip_epoch)\n",
    "list_write.append(Loss_epoch)\n",
    "list_write.append(train_acc_epoch)\n",
    "list_write.append(test_acc_epoch)\n",
    "name = ['Skip', 'Loss', 'train-acc', 'test-acc']\n",
    "test = pd.DataFrame(index=name, data=list_write).T\n",
    "test.to_csv(\"./result/\"+\"LASG-res-cifar10.csv\", encoding='gbk')\n",
    "\n",
    "list_write_iter = []\n",
    "list_write_iter.append(Skip_iter)\n",
    "list_write_iter.append(Loss_iter)\n",
    "list_write_iter.append(train_acc_iter)\n",
    "list_write_iter.append(test_acc_iter)\n",
    "name_iter = ['Skip', 'Loss', 'train-acc', 'test-acc']\n",
    "test = pd.DataFrame(index=name_iter, data=list_write_iter).T\n",
    "test.to_csv(\"./result/\"+\"LASG-res-cifar10-iter.csv\", encoding='gbk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse  lr:0.001--h:0.01--epoch:30--worker:10\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Number_parameter: 62\n",
      "Element_parameter: 11173962\n",
      "EPOCH:  1 learning rate:  0.01\n",
      "Epoch: 001/030 | Batch 0100/0500 | Cost: 1.6553\n",
      "Epoch: 001/030 | Batch 0200/0500 | Cost: 0.9916\n",
      "Epoch: 001/030 | Batch 0300/0500 | Cost: 1.9955\n",
      "Epoch: 001/030 | Batch 0400/0500 | Cost: 1.2333\n",
      "Epoch: 001/030 | Batch 0500/0500 | Cost: 0.9191\n",
      "epoch 1, loss 1.4469, train acc 47.566%, test acc 64.650%\n",
      "EPOCH:  2 learning rate:  0.01\n",
      "Epoch: 002/030 | Batch 0100/0500 | Cost: 0.7892\n",
      "Epoch: 002/030 | Batch 0200/0500 | Cost: 0.8064\n",
      "Epoch: 002/030 | Batch 0300/0500 | Cost: 1.3882\n",
      "Epoch: 002/030 | Batch 0400/0500 | Cost: 1.0509\n",
      "Epoch: 002/030 | Batch 0500/0500 | Cost: 1.2324\n",
      "epoch 2, loss 0.9298, train acc 67.536%, test acc 75.340%\n",
      "EPOCH:  3 learning rate:  0.01\n",
      "Epoch: 003/030 | Batch 0100/0500 | Cost: 0.8624\n",
      "Epoch: 003/030 | Batch 0200/0500 | Cost: 0.5237\n",
      "Epoch: 003/030 | Batch 0300/0500 | Cost: 0.1371\n",
      "Epoch: 003/030 | Batch 0400/0500 | Cost: 0.1085\n",
      "Epoch: 003/030 | Batch 0500/0500 | Cost: 0.4976\n",
      "epoch 3, loss 0.7183, train acc 75.422%, test acc 78.690%\n",
      "EPOCH:  4 learning rate:  0.01\n",
      "Epoch: 004/030 | Batch 0100/0500 | Cost: 0.5481\n",
      "Epoch: 004/030 | Batch 0200/0500 | Cost: 0.4961\n",
      "Epoch: 004/030 | Batch 0300/0500 | Cost: 0.4230\n",
      "Epoch: 004/030 | Batch 0400/0500 | Cost: 1.0010\n",
      "Epoch: 004/030 | Batch 0500/0500 | Cost: 0.3350\n",
      "epoch 4, loss 0.6026, train acc 79.262%, test acc 82.600%\n",
      "EPOCH:  5 learning rate:  0.01\n",
      "Epoch: 005/030 | Batch 0100/0500 | Cost: 0.8246\n",
      "Epoch: 005/030 | Batch 0200/0500 | Cost: 0.3341\n",
      "Epoch: 005/030 | Batch 0300/0500 | Cost: 0.2490\n",
      "Epoch: 005/030 | Batch 0400/0500 | Cost: 0.4187\n",
      "Epoch: 005/030 | Batch 0500/0500 | Cost: 0.1114\n",
      "epoch 5, loss 0.5315, train acc 81.740%, test acc 83.900%\n",
      "EPOCH:  6 learning rate:  0.01\n",
      "Epoch: 006/030 | Batch 0100/0500 | Cost: 0.1687\n",
      "Epoch: 006/030 | Batch 0200/0500 | Cost: 0.1346\n",
      "Epoch: 006/030 | Batch 0300/0500 | Cost: 0.0946\n",
      "Epoch: 006/030 | Batch 0400/0500 | Cost: 0.3125\n",
      "Epoch: 006/030 | Batch 0500/0500 | Cost: 0.2576\n",
      "epoch 6, loss 0.4671, train acc 84.006%, test acc 83.900%\n",
      "EPOCH:  7 learning rate:  0.01\n",
      "Epoch: 007/030 | Batch 0100/0500 | Cost: 0.6162\n",
      "Epoch: 007/030 | Batch 0200/0500 | Cost: 0.5225\n",
      "Epoch: 007/030 | Batch 0300/0500 | Cost: 0.0926\n",
      "Epoch: 007/030 | Batch 0400/0500 | Cost: 0.1236\n",
      "Epoch: 007/030 | Batch 0500/0500 | Cost: 0.5396\n",
      "epoch 7, loss 0.4250, train acc 85.190%, test acc 85.230%\n",
      "EPOCH:  8 learning rate:  0.01\n",
      "Epoch: 008/030 | Batch 0100/0500 | Cost: 0.5250\n",
      "Epoch: 008/030 | Batch 0200/0500 | Cost: 0.3581\n",
      "Epoch: 008/030 | Batch 0300/0500 | Cost: 0.4415\n",
      "Epoch: 008/030 | Batch 0400/0500 | Cost: 0.3156\n",
      "Epoch: 008/030 | Batch 0500/0500 | Cost: 0.7130\n",
      "epoch 8, loss 0.3884, train acc 86.660%, test acc 86.810%\n",
      "EPOCH:  9 learning rate:  0.01\n",
      "Epoch: 009/030 | Batch 0100/0500 | Cost: 0.6304\n",
      "Epoch: 009/030 | Batch 0200/0500 | Cost: 0.0311\n",
      "Epoch: 009/030 | Batch 0300/0500 | Cost: 0.5145\n",
      "Epoch: 009/030 | Batch 0400/0500 | Cost: 0.4810\n",
      "Epoch: 009/030 | Batch 0500/0500 | Cost: 0.0699\n",
      "epoch 9, loss 0.3581, train acc 87.708%, test acc 86.940%\n",
      "EPOCH:  10 learning rate:  0.01\n",
      "Epoch: 010/030 | Batch 0100/0500 | Cost: 0.0761\n",
      "Epoch: 010/030 | Batch 0200/0500 | Cost: 0.5177\n",
      "Epoch: 010/030 | Batch 0300/0500 | Cost: 0.3785\n",
      "Epoch: 010/030 | Batch 0400/0500 | Cost: 0.6527\n",
      "Epoch: 010/030 | Batch 0500/0500 | Cost: 0.2258\n",
      "epoch 10, loss 0.3281, train acc 88.710%, test acc 87.210%\n",
      "EPOCH:  11 learning rate:  0.01\n",
      "Epoch: 011/030 | Batch 0100/0500 | Cost: 0.2628\n",
      "Epoch: 011/030 | Batch 0200/0500 | Cost: 0.4429\n",
      "Epoch: 011/030 | Batch 0300/0500 | Cost: 0.6751\n",
      "Epoch: 011/030 | Batch 0400/0500 | Cost: 0.5482\n",
      "Epoch: 011/030 | Batch 0500/0500 | Cost: 0.3913\n",
      "epoch 11, loss 0.3038, train acc 89.542%, test acc 87.410%\n",
      "EPOCH:  12 learning rate:  0.01\n",
      "Epoch: 012/030 | Batch 0100/0500 | Cost: 0.6122\n",
      "Epoch: 012/030 | Batch 0200/0500 | Cost: 0.2104\n",
      "Epoch: 012/030 | Batch 0300/0500 | Cost: 0.8065\n",
      "Epoch: 012/030 | Batch 0400/0500 | Cost: 0.5450\n",
      "Epoch: 012/030 | Batch 0500/0500 | Cost: 0.4567\n",
      "epoch 12, loss 0.2801, train acc 90.230%, test acc 88.480%\n",
      "EPOCH:  13 learning rate:  0.01\n",
      "Epoch: 013/030 | Batch 0100/0500 | Cost: 0.3230\n",
      "Epoch: 013/030 | Batch 0200/0500 | Cost: 0.5821\n",
      "Epoch: 013/030 | Batch 0300/0500 | Cost: 0.6269\n",
      "Epoch: 013/030 | Batch 0400/0500 | Cost: 0.3979\n",
      "Epoch: 013/030 | Batch 0500/0500 | Cost: 0.5975\n",
      "epoch 13, loss 0.2629, train acc 91.062%, test acc 88.930%\n",
      "EPOCH:  14 learning rate:  0.01\n",
      "Epoch: 014/030 | Batch 0100/0500 | Cost: 0.0607\n",
      "Epoch: 014/030 | Batch 0200/0500 | Cost: 0.1541\n",
      "Epoch: 014/030 | Batch 0300/0500 | Cost: 0.2484\n",
      "Epoch: 014/030 | Batch 0400/0500 | Cost: 0.0416\n",
      "Epoch: 014/030 | Batch 0500/0500 | Cost: 0.0250\n",
      "epoch 14, loss 0.2467, train acc 91.350%, test acc 88.610%\n",
      "EPOCH:  15 learning rate:  0.01\n",
      "Epoch: 015/030 | Batch 0100/0500 | Cost: 0.0237\n",
      "Epoch: 015/030 | Batch 0200/0500 | Cost: 0.5744\n",
      "Epoch: 015/030 | Batch 0300/0500 | Cost: 0.0846\n",
      "Epoch: 015/030 | Batch 0400/0500 | Cost: 0.3442\n",
      "Epoch: 015/030 | Batch 0500/0500 | Cost: 0.1454\n",
      "epoch 15, loss 0.2299, train acc 91.996%, test acc 89.620%\n",
      "EPOCH:  16 learning rate:  0.01\n",
      "Epoch: 016/030 | Batch 0100/0500 | Cost: 0.3624\n",
      "Epoch: 016/030 | Batch 0200/0500 | Cost: 0.6886\n",
      "Epoch: 016/030 | Batch 0300/0500 | Cost: 0.0657\n",
      "Epoch: 016/030 | Batch 0400/0500 | Cost: 0.2470\n",
      "Epoch: 016/030 | Batch 0500/0500 | Cost: 0.0119\n",
      "epoch 16, loss 0.2172, train acc 92.546%, test acc 89.810%\n",
      "EPOCH:  17 learning rate:  0.01\n",
      "Epoch: 017/030 | Batch 0100/0500 | Cost: 0.0666\n",
      "Epoch: 017/030 | Batch 0200/0500 | Cost: 0.0533\n",
      "Epoch: 017/030 | Batch 0300/0500 | Cost: 0.5098\n",
      "Epoch: 017/030 | Batch 0400/0500 | Cost: 0.6008\n",
      "Epoch: 017/030 | Batch 0500/0500 | Cost: 0.3805\n",
      "epoch 17, loss 0.2045, train acc 92.870%, test acc 90.320%\n",
      "EPOCH:  18 learning rate:  0.01\n",
      "Epoch: 018/030 | Batch 0100/0500 | Cost: 0.1126\n",
      "Epoch: 018/030 | Batch 0200/0500 | Cost: 0.0576\n",
      "Epoch: 018/030 | Batch 0300/0500 | Cost: 0.3312\n",
      "Epoch: 018/030 | Batch 0400/0500 | Cost: 0.5739\n",
      "Epoch: 018/030 | Batch 0500/0500 | Cost: 0.2817\n",
      "epoch 18, loss 0.1868, train acc 93.512%, test acc 89.830%\n",
      "EPOCH:  19 learning rate:  0.01\n",
      "Epoch: 019/030 | Batch 0100/0500 | Cost: 0.0413\n",
      "Epoch: 019/030 | Batch 0200/0500 | Cost: 0.2171\n",
      "Epoch: 019/030 | Batch 0300/0500 | Cost: 0.0236\n",
      "Epoch: 019/030 | Batch 0400/0500 | Cost: 0.1392\n",
      "Epoch: 019/030 | Batch 0500/0500 | Cost: 0.2025\n",
      "epoch 19, loss 0.1825, train acc 93.624%, test acc 89.790%\n",
      "EPOCH:  20 learning rate:  0.01\n",
      "Epoch: 020/030 | Batch 0100/0500 | Cost: 0.0684\n",
      "Epoch: 020/030 | Batch 0200/0500 | Cost: 0.0476\n",
      "Epoch: 020/030 | Batch 0300/0500 | Cost: 0.1772\n",
      "Epoch: 020/030 | Batch 0400/0500 | Cost: 0.0795\n",
      "Epoch: 020/030 | Batch 0500/0500 | Cost: 0.0607\n",
      "epoch 20, loss 0.1733, train acc 93.928%, test acc 90.350%\n",
      "EPOCH:  21 learning rate:  0.001\n",
      "Epoch: 021/030 | Batch 0100/0500 | Cost: 0.6893\n",
      "Epoch: 021/030 | Batch 0200/0500 | Cost: 0.2250\n",
      "Epoch: 021/030 | Batch 0300/0500 | Cost: 0.0153\n",
      "Epoch: 021/030 | Batch 0400/0500 | Cost: 0.0258\n",
      "Epoch: 021/030 | Batch 0500/0500 | Cost: 0.0084\n",
      "epoch 21, loss 0.1206, train acc 95.846%, test acc 91.610%\n",
      "EPOCH:  22 learning rate:  0.001\n",
      "Epoch: 022/030 | Batch 0100/0500 | Cost: 0.0904\n",
      "Epoch: 022/030 | Batch 0200/0500 | Cost: 0.0517\n",
      "Epoch: 022/030 | Batch 0300/0500 | Cost: 0.0224\n",
      "Epoch: 022/030 | Batch 0400/0500 | Cost: 0.0154\n",
      "Epoch: 022/030 | Batch 0500/0500 | Cost: 0.0204\n",
      "epoch 22, loss 0.0985, train acc 96.686%, test acc 91.990%\n",
      "EPOCH:  23 learning rate:  0.001\n",
      "Epoch: 023/030 | Batch 0100/0500 | Cost: 0.0022\n",
      "Epoch: 023/030 | Batch 0200/0500 | Cost: 0.2473\n",
      "Epoch: 023/030 | Batch 0300/0500 | Cost: 0.0353\n",
      "****************************************************************************************************\n",
      "Iter_num: 11300 Test_acc 92.12 Comm_round: 113000\n",
      "****************************************************************************************************\n",
      "Epoch: 023/030 | Batch 0400/0500 | Cost: 0.0123\n",
      "Epoch: 023/030 | Batch 0500/0500 | Cost: 0.4289\n",
      "epoch 23, loss 0.0875, train acc 97.182%, test acc 92.130%\n",
      "EPOCH:  24 learning rate:  0.001\n",
      "Epoch: 024/030 | Batch 0100/0500 | Cost: 0.0114\n",
      "Epoch: 024/030 | Batch 0200/0500 | Cost: 0.0497\n",
      "Epoch: 024/030 | Batch 0300/0500 | Cost: 0.0560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 024/030 | Batch 0400/0500 | Cost: 0.0062\n",
      "Epoch: 024/030 | Batch 0500/0500 | Cost: 0.0351\n",
      "epoch 24, loss 0.0806, train acc 97.378%, test acc 92.140%\n",
      "EPOCH:  25 learning rate:  0.001\n",
      "Epoch: 025/030 | Batch 0100/0500 | Cost: 0.0635\n",
      "Epoch: 025/030 | Batch 0200/0500 | Cost: 0.0427\n",
      "Epoch: 025/030 | Batch 0300/0500 | Cost: 0.0573\n",
      "Epoch: 025/030 | Batch 0400/0500 | Cost: 0.1550\n",
      "Epoch: 025/030 | Batch 0500/0500 | Cost: 0.0062\n",
      "epoch 25, loss 0.0772, train acc 97.464%, test acc 92.230%\n",
      "EPOCH:  26 learning rate:  0.001\n",
      "Epoch: 026/030 | Batch 0100/0500 | Cost: 0.0019\n",
      "Epoch: 026/030 | Batch 0200/0500 | Cost: 0.0041\n",
      "Epoch: 026/030 | Batch 0300/0500 | Cost: 0.0547\n",
      "Epoch: 026/030 | Batch 0400/0500 | Cost: 0.1533\n",
      "Epoch: 026/030 | Batch 0500/0500 | Cost: 0.0351\n",
      "epoch 26, loss 0.0738, train acc 97.632%, test acc 92.270%\n",
      "EPOCH:  27 learning rate:  0.001\n",
      "Epoch: 027/030 | Batch 0100/0500 | Cost: 0.0140\n",
      "Epoch: 027/030 | Batch 0200/0500 | Cost: 0.0304\n",
      "Epoch: 027/030 | Batch 0300/0500 | Cost: 0.0653\n",
      "Epoch: 027/030 | Batch 0400/0500 | Cost: 0.0187\n",
      "Epoch: 027/030 | Batch 0500/0500 | Cost: 0.0046\n",
      "epoch 27, loss 0.0717, train acc 97.574%, test acc 92.020%\n",
      "EPOCH:  28 learning rate:  0.001\n",
      "Epoch: 028/030 | Batch 0100/0500 | Cost: 0.0018\n",
      "Epoch: 028/030 | Batch 0200/0500 | Cost: 0.2048\n",
      "Epoch: 028/030 | Batch 0300/0500 | Cost: 0.0021\n",
      "Epoch: 028/030 | Batch 0400/0500 | Cost: 0.0686\n",
      "Epoch: 028/030 | Batch 0500/0500 | Cost: 0.1193\n",
      "epoch 28, loss 0.0684, train acc 97.746%, test acc 92.360%\n",
      "EPOCH:  29 learning rate:  0.001\n",
      "Epoch: 029/030 | Batch 0100/0500 | Cost: 0.0569\n",
      "Epoch: 029/030 | Batch 0200/0500 | Cost: 0.0161\n",
      "Epoch: 029/030 | Batch 0300/0500 | Cost: 0.0026\n",
      "Epoch: 029/030 | Batch 0400/0500 | Cost: 0.0412\n",
      "Epoch: 029/030 | Batch 0500/0500 | Cost: 0.1203\n",
      "epoch 29, loss 0.0650, train acc 97.934%, test acc 92.450%\n",
      "EPOCH:  30 learning rate:  0.001\n",
      "Epoch: 030/030 | Batch 0100/0500 | Cost: 0.0296\n",
      "Epoch: 030/030 | Batch 0200/0500 | Cost: 0.0019\n",
      "Epoch: 030/030 | Batch 0300/0500 | Cost: 0.3583\n",
      "Epoch: 030/030 | Batch 0400/0500 | Cost: 0.0368\n",
      "Epoch: 030/030 | Batch 0500/0500 | Cost: 0.0037\n",
      "epoch 30, loss 0.0647, train acc 97.858%, test acc 92.470%\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "print(\"Sparse  lr:\"+str(lr)+\"--h:\"+str(h)+\"--epoch:\"+str(NUM_EPOCHS)+\"--worker:\"+str(NUM_WORKERS))\n",
    "seed_torch(RANDOM_SEED)\n",
    "model = resnet18()\n",
    "model.to(DEVICE)\n",
    "loss = nn.CrossEntropyLoss()  # 损失函数\n",
    "\n",
    "# prepare data\n",
    "dataset_name = 'cifar10'\n",
    "train_loader_workers, test_loader = create_loaders(dataset_name, NUM_WORKERS, BATCH_SIZE)\n",
    "\n",
    "NUM_PARAS = sum([1 for param in model.parameters()])\n",
    "print(\"Number_parameter:\", NUM_PARAS)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Element_parameter:\", total_params)\n",
    "\n",
    "error_worker = [[0 for col in range(NUM_PARAS)] for row in range(NUM_WORKERS)]\n",
    "Loss_epoch = []\n",
    "train_acc_epoch = []\n",
    "test_acc_epoch = []\n",
    "Loss_iter = []\n",
    "train_acc_iter = []\n",
    "test_acc_iter = []\n",
    "iter_num = 0\n",
    "flag_acc = False\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_l_sum, train_acc_sum, num = 0.0, 0.0, 0\n",
    "    train_loader_iter = [iter(train_loader_workers[w]) for w in range(NUM_WORKERS)]\n",
    "    iter_steps = len(train_loader_workers[0])\n",
    "\n",
    "    # adjust learning_rate\n",
    "    lr, _= adjust_lr(epoch)\n",
    "    print(\"EPOCH: \", epoch + 1, \"learning rate: \", lr)\n",
    "\n",
    "    for batch_idx in range(iter_steps):\n",
    "        model.train()\n",
    "        grad_agg = [0 for col in range(NUM_PARAS)]\n",
    "        for w_id in range(NUM_WORKERS):\n",
    "            images, labels = next(train_loader_iter[w_id])\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            y_hat = model(images)\n",
    "            Loss = loss(y_hat, labels)\n",
    "            Loss.backward()\n",
    "            with torch.no_grad():\n",
    "                p_id = 0\n",
    "                for p in model.parameters():\n",
    "                    g = lr * p.grad.data.clone().detach() + error_worker[w_id][p_id]\n",
    "                    Tk_sparse = top_k_opt(g, h)\n",
    "                    grad_agg[p_id] += Tk_sparse\n",
    "                    error_worker[w_id][p_id] = g - Tk_sparse\n",
    "                    p_id += 1\n",
    "                    p.grad.zero_()\n",
    "            train_l_sum += Loss.item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == labels).sum().item()\n",
    "            num += labels.shape[0]\n",
    "        with torch.no_grad():\n",
    "            p_id = 0\n",
    "            for p in model.parameters():\n",
    "                p.data.add_(grad_agg[p_id], alpha=-1)\n",
    "                p_id += 1\n",
    "                p.grad.zero_()\n",
    "\n",
    "        # LOGGING\n",
    "        if not (batch_idx + 1) % 100:\n",
    "            print('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f'\n",
    "                  % (epoch + 1, NUM_EPOCHS, batch_idx + 1, len(train_loader_workers[0]), Loss.item()))\n",
    "\n",
    "        iter_num += 1\n",
    "        if not iter_num % 100:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_acc_it = evaluate_accuracy(model, test_loader, DEVICE)\n",
    "                Loss_iter.append(train_l_sum / (num / BATCH_SIZE))\n",
    "                train_acc_iter.append(train_acc_sum / num * 100)\n",
    "                test_acc_iter.append(test_acc_it)\n",
    "            if test_acc_it > 92.0 and not flag_acc:\n",
    "                flag_acc = True\n",
    "                print(\"*\" * 100)\n",
    "                print(\"Iter_num:\", iter_num, \"Test_acc\", test_acc_it, \"Comm_round:\", iter_num * 10)  # 10 workers\n",
    "                print(\"*\" * 100)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_acc = evaluate_accuracy(model, test_loader, DEVICE)\n",
    "        Loss_epoch.append(train_l_sum / (num / BATCH_SIZE))\n",
    "        train_acc_epoch.append(train_acc_sum / num * 100)\n",
    "        test_acc_epoch.append(test_acc)\n",
    "    print('epoch %d, loss %.4f, train acc %.3f%%, test acc %.3f%%'\n",
    "          % (epoch + 1, train_l_sum / (num / BATCH_SIZE), train_acc_sum / num * 100, test_acc))\n",
    "\n",
    "print('Finished.')\n",
    "\n",
    "list_write = []\n",
    "list_write.append(Loss_epoch)\n",
    "list_write.append(train_acc_epoch)\n",
    "list_write.append(test_acc_epoch)\n",
    "name = ['Loss', 'train-acc', 'test-acc']\n",
    "test = pd.DataFrame(index=name, data=list_write).T\n",
    "test.to_csv(\"./result/\"+\"Sparse-res-cifar10.csv\", encoding='gbk')\n",
    "\n",
    "\n",
    "list_write_iter = []\n",
    "list_write_iter.append(Loss_iter)\n",
    "list_write_iter.append(train_acc_iter)\n",
    "list_write_iter.append(test_acc_iter)\n",
    "name_iter = ['Loss', 'train-acc', 'test-acc']\n",
    "test = pd.DataFrame(index=name_iter, data=list_write_iter).T\n",
    "test.to_csv(\"./result/\"+\"Sparse-res-cifar10-iter.csv\", encoding='gbk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dis-SGD  lr:0.001--epoch:30--worker:10\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Number_parameter: 62\n",
      "Element_parameter: 11173962\n",
      "EPOCH:  1 learning rate:  0.01\n",
      "Epoch: 001/030 | Batch 0100/0500 | Cost: 1.5711\n",
      "Epoch: 001/030 | Batch 0200/0500 | Cost: 1.7554\n",
      "Epoch: 001/030 | Batch 0300/0500 | Cost: 2.0835\n",
      "Epoch: 001/030 | Batch 0400/0500 | Cost: 1.6002\n",
      "Epoch: 001/030 | Batch 0500/0500 | Cost: 1.2747\n",
      "epoch 1, loss 1.8287, train acc 34.600%, test acc 55.200%\n",
      "EPOCH:  2 learning rate:  0.01\n",
      "Epoch: 002/030 | Batch 0100/0500 | Cost: 0.8878\n",
      "Epoch: 002/030 | Batch 0200/0500 | Cost: 1.3493\n",
      "Epoch: 002/030 | Batch 0300/0500 | Cost: 1.6958\n",
      "Epoch: 002/030 | Batch 0400/0500 | Cost: 1.3677\n",
      "Epoch: 002/030 | Batch 0500/0500 | Cost: 1.8068\n",
      "epoch 2, loss 1.2500, train acc 55.572%, test acc 61.420%\n",
      "EPOCH:  3 learning rate:  0.01\n",
      "Epoch: 003/030 | Batch 0100/0500 | Cost: 0.6893\n",
      "Epoch: 003/030 | Batch 0200/0500 | Cost: 1.2035\n",
      "Epoch: 003/030 | Batch 0300/0500 | Cost: 0.3772\n",
      "Epoch: 003/030 | Batch 0400/0500 | Cost: 0.4183\n",
      "Epoch: 003/030 | Batch 0500/0500 | Cost: 0.6447\n",
      "epoch 3, loss 0.9552, train acc 66.708%, test acc 73.230%\n",
      "EPOCH:  4 learning rate:  0.01\n",
      "Epoch: 004/030 | Batch 0100/0500 | Cost: 0.7222\n",
      "Epoch: 004/030 | Batch 0200/0500 | Cost: 0.3992\n",
      "Epoch: 004/030 | Batch 0300/0500 | Cost: 0.7863\n",
      "Epoch: 004/030 | Batch 0400/0500 | Cost: 0.9019\n",
      "Epoch: 004/030 | Batch 0500/0500 | Cost: 0.4070\n",
      "epoch 4, loss 0.7774, train acc 72.994%, test acc 76.780%\n",
      "EPOCH:  5 learning rate:  0.01\n",
      "Epoch: 005/030 | Batch 0100/0500 | Cost: 0.7412\n",
      "Epoch: 005/030 | Batch 0200/0500 | Cost: 0.4946\n",
      "Epoch: 005/030 | Batch 0300/0500 | Cost: 0.3787\n",
      "Epoch: 005/030 | Batch 0400/0500 | Cost: 0.5309\n",
      "Epoch: 005/030 | Batch 0500/0500 | Cost: 0.4841\n",
      "epoch 5, loss 0.6731, train acc 76.846%, test acc 80.490%\n",
      "EPOCH:  6 learning rate:  0.01\n",
      "Epoch: 006/030 | Batch 0100/0500 | Cost: 0.9026\n",
      "Epoch: 006/030 | Batch 0200/0500 | Cost: 0.2940\n",
      "Epoch: 006/030 | Batch 0300/0500 | Cost: 0.1012\n",
      "Epoch: 006/030 | Batch 0400/0500 | Cost: 0.2204\n",
      "Epoch: 006/030 | Batch 0500/0500 | Cost: 0.2029\n",
      "epoch 6, loss 0.5895, train acc 79.768%, test acc 82.060%\n",
      "EPOCH:  7 learning rate:  0.01\n",
      "Epoch: 007/030 | Batch 0100/0500 | Cost: 0.7483\n",
      "Epoch: 007/030 | Batch 0200/0500 | Cost: 0.6617\n",
      "Epoch: 007/030 | Batch 0300/0500 | Cost: 0.1231\n",
      "Epoch: 007/030 | Batch 0400/0500 | Cost: 0.2795\n",
      "Epoch: 007/030 | Batch 0500/0500 | Cost: 0.8708\n",
      "epoch 7, loss 0.5331, train acc 81.812%, test acc 81.750%\n",
      "EPOCH:  8 learning rate:  0.01\n",
      "Epoch: 008/030 | Batch 0100/0500 | Cost: 0.7550\n",
      "Epoch: 008/030 | Batch 0200/0500 | Cost: 0.6540\n",
      "Epoch: 008/030 | Batch 0300/0500 | Cost: 0.2594\n",
      "Epoch: 008/030 | Batch 0400/0500 | Cost: 0.4048\n",
      "Epoch: 008/030 | Batch 0500/0500 | Cost: 0.7421\n",
      "epoch 8, loss 0.4875, train acc 83.362%, test acc 84.730%\n",
      "EPOCH:  9 learning rate:  0.01\n",
      "Epoch: 009/030 | Batch 0100/0500 | Cost: 0.5742\n",
      "Epoch: 009/030 | Batch 0200/0500 | Cost: 0.1447\n",
      "Epoch: 009/030 | Batch 0300/0500 | Cost: 0.4031\n",
      "Epoch: 009/030 | Batch 0400/0500 | Cost: 0.6177\n",
      "Epoch: 009/030 | Batch 0500/0500 | Cost: 0.1598\n",
      "epoch 9, loss 0.4471, train acc 84.808%, test acc 86.330%\n",
      "EPOCH:  10 learning rate:  0.01\n",
      "Epoch: 010/030 | Batch 0100/0500 | Cost: 0.2874\n",
      "Epoch: 010/030 | Batch 0200/0500 | Cost: 0.5831\n",
      "Epoch: 010/030 | Batch 0300/0500 | Cost: 0.2571\n",
      "Epoch: 010/030 | Batch 0400/0500 | Cost: 0.9490\n",
      "Epoch: 010/030 | Batch 0500/0500 | Cost: 0.2412\n",
      "epoch 10, loss 0.4063, train acc 86.194%, test acc 86.770%\n",
      "EPOCH:  11 learning rate:  0.01\n",
      "Epoch: 011/030 | Batch 0100/0500 | Cost: 0.3398\n",
      "Epoch: 011/030 | Batch 0200/0500 | Cost: 0.3171\n",
      "Epoch: 011/030 | Batch 0300/0500 | Cost: 0.5533\n",
      "Epoch: 011/030 | Batch 0400/0500 | Cost: 0.7451\n",
      "Epoch: 011/030 | Batch 0500/0500 | Cost: 0.7717\n",
      "epoch 11, loss 0.3816, train acc 86.920%, test acc 84.030%\n",
      "EPOCH:  12 learning rate:  0.01\n",
      "Epoch: 012/030 | Batch 0100/0500 | Cost: 0.6951\n",
      "Epoch: 012/030 | Batch 0200/0500 | Cost: 0.1869\n",
      "Epoch: 012/030 | Batch 0300/0500 | Cost: 0.6401\n",
      "Epoch: 012/030 | Batch 0400/0500 | Cost: 0.6694\n",
      "Epoch: 012/030 | Batch 0500/0500 | Cost: 0.6144\n",
      "epoch 12, loss 0.3509, train acc 88.036%, test acc 87.100%\n",
      "EPOCH:  13 learning rate:  0.01\n",
      "Epoch: 013/030 | Batch 0100/0500 | Cost: 0.3661\n",
      "Epoch: 013/030 | Batch 0200/0500 | Cost: 0.7450\n",
      "Epoch: 013/030 | Batch 0300/0500 | Cost: 0.5001\n",
      "Epoch: 013/030 | Batch 0400/0500 | Cost: 0.6657\n",
      "Epoch: 013/030 | Batch 0500/0500 | Cost: 0.3060\n",
      "epoch 13, loss 0.3314, train acc 88.730%, test acc 86.210%\n",
      "EPOCH:  14 learning rate:  0.01\n",
      "Epoch: 014/030 | Batch 0100/0500 | Cost: 0.0754\n",
      "Epoch: 014/030 | Batch 0200/0500 | Cost: 0.4852\n",
      "Epoch: 014/030 | Batch 0300/0500 | Cost: 0.2479\n",
      "Epoch: 014/030 | Batch 0400/0500 | Cost: 0.1586\n",
      "Epoch: 014/030 | Batch 0500/0500 | Cost: 0.2022\n",
      "epoch 14, loss 0.3135, train acc 89.198%, test acc 86.370%\n",
      "EPOCH:  15 learning rate:  0.01\n",
      "Epoch: 015/030 | Batch 0100/0500 | Cost: 0.1493\n",
      "Epoch: 015/030 | Batch 0200/0500 | Cost: 0.8090\n",
      "Epoch: 015/030 | Batch 0300/0500 | Cost: 0.1087\n",
      "Epoch: 015/030 | Batch 0400/0500 | Cost: 0.5230\n",
      "Epoch: 015/030 | Batch 0500/0500 | Cost: 0.1706\n",
      "epoch 15, loss 0.2882, train acc 89.964%, test acc 87.340%\n",
      "EPOCH:  16 learning rate:  0.01\n",
      "Epoch: 016/030 | Batch 0100/0500 | Cost: 0.1949\n",
      "Epoch: 016/030 | Batch 0200/0500 | Cost: 0.5143\n",
      "Epoch: 016/030 | Batch 0300/0500 | Cost: 0.3055\n",
      "Epoch: 016/030 | Batch 0400/0500 | Cost: 0.1084\n",
      "Epoch: 016/030 | Batch 0500/0500 | Cost: 0.0200\n",
      "epoch 16, loss 0.2743, train acc 90.518%, test acc 87.010%\n",
      "EPOCH:  17 learning rate:  0.01\n",
      "Epoch: 017/030 | Batch 0100/0500 | Cost: 0.1439\n",
      "Epoch: 017/030 | Batch 0200/0500 | Cost: 0.0164\n",
      "Epoch: 017/030 | Batch 0300/0500 | Cost: 0.2744\n",
      "Epoch: 017/030 | Batch 0400/0500 | Cost: 0.3715\n",
      "Epoch: 017/030 | Batch 0500/0500 | Cost: 0.7499\n",
      "epoch 17, loss 0.2590, train acc 91.076%, test acc 85.560%\n",
      "EPOCH:  18 learning rate:  0.01\n",
      "Epoch: 018/030 | Batch 0100/0500 | Cost: 0.2656\n",
      "Epoch: 018/030 | Batch 0200/0500 | Cost: 0.1682\n",
      "Epoch: 018/030 | Batch 0300/0500 | Cost: 0.2261\n",
      "Epoch: 018/030 | Batch 0400/0500 | Cost: 0.7829\n",
      "Epoch: 018/030 | Batch 0500/0500 | Cost: 0.2102\n",
      "epoch 18, loss 0.2418, train acc 91.500%, test acc 88.330%\n",
      "EPOCH:  19 learning rate:  0.01\n",
      "Epoch: 019/030 | Batch 0100/0500 | Cost: 0.0533\n",
      "Epoch: 019/030 | Batch 0200/0500 | Cost: 0.5430\n",
      "Epoch: 019/030 | Batch 0300/0500 | Cost: 0.0701\n",
      "Epoch: 019/030 | Batch 0400/0500 | Cost: 0.0301\n",
      "Epoch: 019/030 | Batch 0500/0500 | Cost: 0.3013\n",
      "epoch 19, loss 0.2306, train acc 92.000%, test acc 88.590%\n",
      "EPOCH:  20 learning rate:  0.01\n",
      "Epoch: 020/030 | Batch 0100/0500 | Cost: 0.3634\n",
      "Epoch: 020/030 | Batch 0200/0500 | Cost: 0.0853\n",
      "Epoch: 020/030 | Batch 0300/0500 | Cost: 0.4247\n",
      "Epoch: 020/030 | Batch 0400/0500 | Cost: 0.0617\n",
      "Epoch: 020/030 | Batch 0500/0500 | Cost: 0.0758\n",
      "epoch 20, loss 0.2167, train acc 92.592%, test acc 89.810%\n",
      "EPOCH:  21 learning rate:  0.001\n",
      "Epoch: 021/030 | Batch 0100/0500 | Cost: 0.7982\n",
      "Epoch: 021/030 | Batch 0200/0500 | Cost: 0.0698\n",
      "Epoch: 021/030 | Batch 0300/0500 | Cost: 0.1192\n",
      "Epoch: 021/030 | Batch 0400/0500 | Cost: 0.0107\n",
      "Epoch: 021/030 | Batch 0500/0500 | Cost: 0.0463\n",
      "epoch 21, loss 0.1428, train acc 95.172%, test acc 91.650%\n",
      "EPOCH:  22 learning rate:  0.001\n",
      "Epoch: 022/030 | Batch 0100/0500 | Cost: 0.0109\n",
      "Epoch: 022/030 | Batch 0200/0500 | Cost: 0.0150\n",
      "Epoch: 022/030 | Batch 0300/0500 | Cost: 0.0510\n",
      "Epoch: 022/030 | Batch 0400/0500 | Cost: 0.0245\n",
      "Epoch: 022/030 | Batch 0500/0500 | Cost: 0.0782\n",
      "epoch 22, loss 0.1248, train acc 95.844%, test acc 91.860%\n",
      "EPOCH:  23 learning rate:  0.001\n",
      "Epoch: 023/030 | Batch 0100/0500 | Cost: 0.0175\n",
      "Epoch: 023/030 | Batch 0200/0500 | Cost: 0.3344\n",
      "Epoch: 023/030 | Batch 0300/0500 | Cost: 0.0768\n",
      "Epoch: 023/030 | Batch 0400/0500 | Cost: 0.0216\n",
      "Epoch: 023/030 | Batch 0500/0500 | Cost: 0.4020\n",
      "epoch 23, loss 0.1135, train acc 96.172%, test acc 91.930%\n",
      "EPOCH:  24 learning rate:  0.001\n",
      "Epoch: 024/030 | Batch 0100/0500 | Cost: 0.0089\n",
      "Epoch: 024/030 | Batch 0200/0500 | Cost: 0.0237\n",
      "Epoch: 024/030 | Batch 0300/0500 | Cost: 0.0559\n",
      "Epoch: 024/030 | Batch 0400/0500 | Cost: 0.0070\n",
      "Epoch: 024/030 | Batch 0500/0500 | Cost: 0.0610\n",
      "****************************************************************************************************\n",
      "Iter_num: 12000 Test_acc 92.02 Comm_round: 120000\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 24, loss 0.1098, train acc 96.264%, test acc 92.020%\n",
      "EPOCH:  25 learning rate:  0.001\n",
      "Epoch: 025/030 | Batch 0100/0500 | Cost: 0.1842\n",
      "Epoch: 025/030 | Batch 0200/0500 | Cost: 0.0269\n",
      "Epoch: 025/030 | Batch 0300/0500 | Cost: 0.1695\n",
      "Epoch: 025/030 | Batch 0400/0500 | Cost: 0.1533\n",
      "Epoch: 025/030 | Batch 0500/0500 | Cost: 0.0137\n",
      "epoch 25, loss 0.1052, train acc 96.462%, test acc 92.130%\n",
      "EPOCH:  26 learning rate:  0.001\n",
      "Epoch: 026/030 | Batch 0100/0500 | Cost: 0.0127\n",
      "Epoch: 026/030 | Batch 0200/0500 | Cost: 0.0082\n",
      "Epoch: 026/030 | Batch 0300/0500 | Cost: 0.1978\n",
      "Epoch: 026/030 | Batch 0400/0500 | Cost: 0.2362\n",
      "Epoch: 026/030 | Batch 0500/0500 | Cost: 0.0482\n",
      "epoch 26, loss 0.1021, train acc 96.594%, test acc 92.040%\n",
      "EPOCH:  27 learning rate:  0.001\n",
      "Epoch: 027/030 | Batch 0100/0500 | Cost: 0.0974\n",
      "Epoch: 027/030 | Batch 0200/0500 | Cost: 0.2017\n",
      "Epoch: 027/030 | Batch 0300/0500 | Cost: 0.1389\n",
      "Epoch: 027/030 | Batch 0400/0500 | Cost: 0.0452\n",
      "Epoch: 027/030 | Batch 0500/0500 | Cost: 0.0065\n",
      "epoch 27, loss 0.0985, train acc 96.600%, test acc 92.070%\n",
      "EPOCH:  28 learning rate:  0.001\n",
      "Epoch: 028/030 | Batch 0100/0500 | Cost: 0.0030\n",
      "Epoch: 028/030 | Batch 0200/0500 | Cost: 0.1091\n",
      "Epoch: 028/030 | Batch 0300/0500 | Cost: 0.0028\n",
      "Epoch: 028/030 | Batch 0400/0500 | Cost: 0.0423\n",
      "Epoch: 028/030 | Batch 0500/0500 | Cost: 0.1398\n",
      "epoch 28, loss 0.0946, train acc 96.810%, test acc 92.280%\n",
      "EPOCH:  29 learning rate:  0.001\n",
      "Epoch: 029/030 | Batch 0100/0500 | Cost: 0.2250\n",
      "Epoch: 029/030 | Batch 0200/0500 | Cost: 0.0198\n",
      "Epoch: 029/030 | Batch 0300/0500 | Cost: 0.0156\n",
      "Epoch: 029/030 | Batch 0400/0500 | Cost: 0.0575\n",
      "Epoch: 029/030 | Batch 0500/0500 | Cost: 0.1887\n",
      "epoch 29, loss 0.0906, train acc 96.932%, test acc 92.090%\n",
      "EPOCH:  30 learning rate:  0.001\n",
      "Epoch: 030/030 | Batch 0100/0500 | Cost: 0.0279\n",
      "Epoch: 030/030 | Batch 0200/0500 | Cost: 0.0066\n",
      "Epoch: 030/030 | Batch 0300/0500 | Cost: 0.0487\n",
      "Epoch: 030/030 | Batch 0400/0500 | Cost: 0.0589\n",
      "Epoch: 030/030 | Batch 0500/0500 | Cost: 0.0451\n",
      "epoch 30, loss 0.0872, train acc 97.050%, test acc 92.120%\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "print(\"Dis-SGD  lr:\"+str(lr)+\"--epoch:\"+str(NUM_EPOCHS)+\"--worker:\"+str(NUM_WORKERS))\n",
    "seed_torch(RANDOM_SEED)\n",
    "model = resnet18()\n",
    "model.to(DEVICE)\n",
    "loss = nn.CrossEntropyLoss()  # 损失函数\n",
    "\n",
    "# prepare data\n",
    "dataset_name = 'cifar10'\n",
    "train_loader_workers, test_loader = create_loaders(dataset_name, NUM_WORKERS, BATCH_SIZE)\n",
    "\n",
    "NUM_PARAS = sum([1 for param in model.parameters()])\n",
    "print(\"Number_parameter:\", NUM_PARAS)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Element_parameter:\", total_params)\n",
    "\n",
    "Loss_epoch = []\n",
    "train_acc_epoch = []\n",
    "test_acc_epoch = []\n",
    "Loss_iter = []\n",
    "train_acc_iter = []\n",
    "test_acc_iter = []\n",
    "iter_num = 0\n",
    "flag_acc = False\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_l_sum, train_acc_sum, num = 0.0, 0.0, 0\n",
    "    train_loader_iter = [iter(train_loader_workers[w]) for w in range(NUM_WORKERS)]\n",
    "    iter_steps = len(train_loader_workers[0])\n",
    "\n",
    "    # adjust learning_rate\n",
    "    lr, _= adjust_lr(epoch)\n",
    "    print(\"EPOCH: \", epoch + 1, \"learning rate: \", lr)\n",
    "\n",
    "    for batch_idx in range(iter_steps):\n",
    "        model.train()\n",
    "        grad_agg = [0 for col in range(NUM_PARAS)]\n",
    "        for w_id in range(NUM_WORKERS):\n",
    "            images, labels = next(train_loader_iter[w_id])\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            y_hat = model(images)\n",
    "            Loss = loss(y_hat, labels)\n",
    "            Loss.backward()\n",
    "            with torch.no_grad():\n",
    "                p_id = 0\n",
    "                for p in model.parameters():\n",
    "                    grad_agg[p_id] += lr * p.grad.data.clone().detach()\n",
    "                    p_id += 1\n",
    "                    p.grad.zero_()\n",
    "            train_l_sum += Loss.item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == labels).sum().item()\n",
    "            num += labels.shape[0]\n",
    "        with torch.no_grad():\n",
    "            p_id = 0\n",
    "            for p in model.parameters():\n",
    "                p.data.add_(grad_agg[p_id], alpha=-1)\n",
    "                p_id += 1\n",
    "                p.grad.zero_()\n",
    "        # LOGGING\n",
    "        if not (batch_idx + 1) % 100:\n",
    "            print('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f'\n",
    "                  % (epoch + 1, NUM_EPOCHS, batch_idx + 1, len(train_loader_workers[0]), Loss.item()))\n",
    "        iter_num += 1\n",
    "        if not iter_num % 100:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                test_acc_it = evaluate_accuracy(model, test_loader, DEVICE)\n",
    "                Loss_iter.append(train_l_sum / (num / BATCH_SIZE))\n",
    "                train_acc_iter.append(train_acc_sum / num * 100)\n",
    "                test_acc_iter.append(test_acc_it)\n",
    "            if test_acc_it > 92.0 and not flag_acc:\n",
    "                flag_acc = True\n",
    "                print(\"*\" * 100)\n",
    "                print(\"Iter_num:\", iter_num, \"Test_acc\", test_acc_it, \"Comm_round:\", iter_num * 10)  # 10 workers\n",
    "                print(\"*\" * 100)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_acc = evaluate_accuracy(model, test_loader, DEVICE)\n",
    "        Loss_epoch.append(train_l_sum / (num / BATCH_SIZE))\n",
    "        train_acc_epoch.append(train_acc_sum / num * 100)\n",
    "        test_acc_epoch.append(test_acc)\n",
    "    print('epoch %d, loss %.4f, train acc %.3f%%, test acc %.3f%%'\n",
    "          % (epoch + 1, train_l_sum / (num / BATCH_SIZE), train_acc_sum / num * 100, test_acc))\n",
    "\n",
    "print('Finished.')\n",
    "\n",
    "list_write = []\n",
    "list_write.append(Loss_epoch)\n",
    "list_write.append(train_acc_epoch)\n",
    "list_write.append(test_acc_epoch)\n",
    "name = ['Loss', 'train-acc', 'test-acc']\n",
    "test = pd.DataFrame(index=name, data=list_write).T\n",
    "test.to_csv(\"./result/\"+\"SGD-res-cifar10.csv\", encoding='gbk')\n",
    "\n",
    "list_write_iter = []\n",
    "list_write_iter.append(Loss_iter)\n",
    "list_write_iter.append(train_acc_iter)\n",
    "list_write_iter.append(test_acc_iter)\n",
    "name_iter = ['Loss', 'train-acc', 'test-acc']\n",
    "test = pd.DataFrame(index=name_iter, data=list_write_iter).T\n",
    "test.to_csv(\"./result/\"+\"SGD-res-cifar10-iter.csv\", encoding='gbk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original communication number: 1000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Record once every 100 iterations, and 10 workers need to communicate in each iteration\n",
    "original_num = 1000\n",
    "print(\"original communication number:\", original_num)\n",
    "\n",
    "SASG_data = pd.read_csv(\"./result/\"+\"SASG-res-cifar10-iter.csv\")\n",
    "LASG_data = pd.read_csv(\"./result/\"+\"LASG-res-cifar10-iter.csv\")\n",
    "sparse_data = pd.read_csv(\"./result/\"+\"Sparse-res-cifar10-iter.csv\")\n",
    "sgd_data = pd.read_csv(\"./result/\"+\"SGD-res-cifar10-iter.csv\")\n",
    "\n",
    "SASG_skip = SASG_data['Skip'].values.tolist()\n",
    "LASG_skip = LASG_data['Skip'].values.tolist()\n",
    "\n",
    "comm_round_SASG = []\n",
    "comm_round_LASG = []\n",
    "comm_round_sparse = []\n",
    "comm_round_dis = []\n",
    "comm_bit_SASG = []\n",
    "comm_bit_LASG = []\n",
    "comm_bit_sparse = []\n",
    "comm_bit_dis = []\n",
    "\n",
    "comm_num_SASG, comm_num_LASG, comm_num_sparse, comm_num_dis = 0, 0, 0, 0\n",
    "for i in range(len(SASG_skip)):\n",
    "    comm_num_SASG += original_num\n",
    "    comm_num_LASG += original_num\n",
    "    comm_num_sparse += original_num\n",
    "    comm_num_dis += original_num\n",
    "    comm_round_SASG.append(comm_num_SASG - SASG_skip[i])\n",
    "    comm_round_LASG.append(comm_num_LASG - LASG_skip[i])\n",
    "    comm_round_sparse.append(comm_num_sparse)\n",
    "    comm_round_dis.append(comm_num_dis)\n",
    "\n",
    "font1 = {'weight': 'normal', 'size': 17}\n",
    "font2 = {'weight': 'normal', 'size': 20}\n",
    "plt.figure()\n",
    "plt.rc('font', size=17)\n",
    "plt.subplot(facecolor=\"whitesmoke\")\n",
    "plt.grid(axis=\"x\", color='w', linestyle='-', which='major', linewidth=1.2)\n",
    "plt.grid(axis=\"y\", color='w', linestyle='-', which='both', linewidth=1.2)\n",
    "plt.plot(comm_round_dis, sgd_data['test-acc'].values.tolist(), 'black', label='SGD', linewidth=2.5)\n",
    "plt.plot(comm_round_sparse, sparse_data['test-acc'].values.tolist(), 'b', label='Sparse', linewidth=2.5)\n",
    "plt.plot(comm_round_LASG, LASG_data['test-acc'].values.tolist(), 'g', label='LASG', linewidth=2.5)\n",
    "plt.plot(comm_round_SASG, SASG_data['test-acc'].values.tolist(), 'r', label='SASG', linewidth=2.5)\n",
    "ax = plt.gca()\n",
    "xmajorLocator = MultipleLocator(50000)  # major\n",
    "ax.xaxis.set_major_locator(xmajorLocator)\n",
    "xminorLocator = MultipleLocator(25000)  # minor\n",
    "ax.xaxis.set_minor_locator(xminorLocator)\n",
    "ax.ticklabel_format(axis='x', style='scientific', scilimits=(0, 0))\n",
    "plt.xlabel('Communication Round', font2)\n",
    "plt.ylim(30, 95)\n",
    "ymajorLocator = MultipleLocator(15)  # major\n",
    "ax.yaxis.set_major_locator(ymajorLocator)\n",
    "yminorLocator = MultipleLocator(15)  # minor\n",
    "ax.yaxis.set_minor_locator(yminorLocator)\n",
    "plt.ylabel('Test Accuracy', font2)\n",
    "legend = plt.legend(prop=font1)\n",
    "plt.subplots_adjust(left=0.13, right=0.95, bottom=0.15, top=0.95)\n",
    "plt.savefig(\"./result/\"+\"test_res_cifar10.png\", dpi=600)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.rc('font', size=17)\n",
    "plt.subplot(facecolor=\"whitesmoke\")\n",
    "plt.grid(axis=\"x\", color='w', linestyle='-', which='major', linewidth=1.2)\n",
    "plt.grid(axis=\"y\", color='w', linestyle='-', which='major', linewidth=1.2)\n",
    "plt.plot(comm_round_dis, sgd_data['Loss'].values.tolist(), 'black', label='SGD', linewidth=2.5)\n",
    "plt.plot(comm_round_sparse, sparse_data['Loss'].values.tolist(), 'b', label='Sparse', linewidth=2.5)\n",
    "plt.plot(comm_round_LASG, LASG_data['Loss'].values.tolist(), 'g', label='LASG', linewidth=2.5)\n",
    "plt.plot(comm_round_SASG, SASG_data['Loss'].values.tolist(), 'r', label='SASG', linewidth=2.5)\n",
    "ax = plt.gca()\n",
    "xmajorLocator = MultipleLocator(50000)  # major\n",
    "ax.xaxis.set_major_locator(xmajorLocator)\n",
    "xminorLocator = MultipleLocator(25000)  # minor\n",
    "ax.xaxis.set_minor_locator(xminorLocator)\n",
    "ax.ticklabel_format(axis='x', style='scientific', scilimits=(0, 0))\n",
    "plt.xlabel('Communication Round', font2)\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Loss', font2)\n",
    "legend = plt.legend(prop=font1)\n",
    "plt.subplots_adjust(left=0.16, right=0.95, bottom=0.15, top=0.95)\n",
    "plt.savefig(\"./result/\"+\"loss_res_cifar10.png\", dpi=600)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
